{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65ac2cb2",
   "metadata": {},
   "source": [
    "# Interactive Benchmark Segmentation Analysis\n",
    "\n",
    "use this notebook to interactively explore segmentation results from the benchmark. \n",
    "It connects to the local MLflow database to query run metadata and loads artifacts from the specified remote mount point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde5791c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1769448303.522392  138750 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1769448303.526149  138750 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import mlflow\n",
    "import yaml\n",
    "\n",
    "# Add current directory and src to path\n",
    "sys.path.append(os.getcwd())\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "from benchmark_db import BenchmarkDataManager\n",
    "# Try importing the loader function from source\n",
    "try:\n",
    "    from tsseg_exp.datasets.loaders import load_dataset\n",
    "except ImportError:\n",
    "    print(\"Warning: Could not import load_dataset from tsseg_exp.datasets.loaders. Ensure ../src is in path.\")\n",
    "    # Define a dummy loader to prevent crash if not found\n",
    "    def load_dataset(*args, **kwargs):\n",
    "        return None, None\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ac610cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for Artifact Root...\n",
      "  Checking: /home/fchavell/tsseg-project/tsseg-exp/mlartifacts -> Found\n",
      "\n",
      "Tracking URI: sqlite:////home/fchavell/tsseg-project/tsseg-exp/results/mlflow_snapshot.db\n",
      "Selected Artifact Root: /home/fchavell/tsseg-project/tsseg-exp/mlartifacts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/26 18:25:06 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2026/01/26 18:25:06 INFO mlflow.store.db.utils: Updating database tables\n",
      "2026/01/26 18:25:06 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/01/26 18:25:06 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2026/01/26 18:25:06 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/01/26 18:25:06 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "MLFLOW_DB_PATH = PROJECT_ROOT / \"results/mlflow_snapshot.db\"\n",
    "TRACKING_URI = f\"sqlite:///{MLFLOW_DB_PATH}\"\n",
    "CONFIG_PATH = PROJECT_ROOT / \"configs/benchmark_config.yaml\"\n",
    "\n",
    "# Auto-detect Artifact Path\n",
    "potential_roots = []\n",
    "uid = os.getuid()\n",
    "\n",
    "# 1. GVFS Mounts (Priority)\n",
    "gvfs_root = Path(f\"/run/user/{uid}/gvfs\")\n",
    "if gvfs_root.exists():\n",
    "    candidate_mounts = list(gvfs_root.glob(\"sftp*cleps*\"))\n",
    "    for mount in candidate_mounts:\n",
    "        # Variant A: Mount is Root (path includes home)\n",
    "        potential_roots.append(mount / \"home/fchavell/scratch/tsseg-exp/mlartifacts\")\n",
    "        # Variant B: Mount is Home (path starts at scratch)\n",
    "        potential_roots.append(mount / \"scratch/tsseg-exp/mlartifacts\")\n",
    "\n",
    "# 2. Local Fallback\n",
    "potential_roots.append(PROJECT_ROOT / \"mlartifacts\")\n",
    "\n",
    "ARTIFACT_ROOT = None\n",
    "print(\"Searching for Artifact Root...\")\n",
    "for p in potential_roots:\n",
    "    try:\n",
    "        if p.exists():\n",
    "            print(f\"  Checking: {p} -> Found\")\n",
    "            if ARTIFACT_ROOT is None:\n",
    "                ARTIFACT_ROOT = p\n",
    "        else:\n",
    "            print(f\"  Checking: {p} -> Missing\")\n",
    "    except PermissionError:\n",
    "        print(f\"  Checking: {p} -> Permission Denied\")\n",
    "\n",
    "if ARTIFACT_ROOT is None:\n",
    "    print(\"Warning: No accessible artifact root found. Defaulting to local path.\")\n",
    "    ARTIFACT_ROOT = PROJECT_ROOT / \"mlartifacts\"\n",
    "\n",
    "print(f\"\\nTracking URI: {TRACKING_URI}\")\n",
    "print(f\"Selected Artifact Root: {ARTIFACT_ROOT}\")\n",
    "\n",
    "# --- Initialize Manager & Analyzer ---\n",
    "# We now use the robust classes from the paper reproduction\n",
    "from mlflow_manager import MLflowBenchmarkManager\n",
    "from benchmark_analysis import BenchmarkAnalyzer\n",
    "\n",
    "# Initialize Manager with explicit Tracking URI & Config\n",
    "manager = MLflowBenchmarkManager(CONFIG_PATH, tracking_uri=TRACKING_URI)\n",
    "analyzer = BenchmarkAnalyzer(manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c65b4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676e7afac4c74c4c80bdabcf128c762a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Mode:', options=('default', 'guided', 'grid_default', 'grid_guided'), valâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "class SegmentationDashboard:\n",
    "    def __init__(self, analyzer, artifact_root):\n",
    "        self.analyzer = analyzer\n",
    "        self.manager = analyzer.manager\n",
    "        self.artifact_root = Path(artifact_root)\n",
    "        \n",
    "        # State Flags\n",
    "        self._updating = False\n",
    "        \n",
    "        # Data Cache\n",
    "        self.df_parents = pd.DataFrame()\n",
    "        self.df_children = pd.DataFrame()\n",
    "        \n",
    "        # --- UI Components ---\n",
    "        self.modes = ['default', 'guided', 'grid_default', 'grid_guided']\n",
    "        self.dropdown_mode = widgets.Dropdown(\n",
    "            options=self.modes, \n",
    "            description=\"Mode:\",\n",
    "            value=self.modes[0]\n",
    "        )\n",
    "        \n",
    "        self.dropdown_algo = widgets.Dropdown(description=\"Algorithm:\")\n",
    "        self.dropdown_dataset = widgets.Dropdown(description=\"Dataset:\")\n",
    "        self.dropdown_series = widgets.Dropdown(description=\"Series:\", layout={'width': 'max-content'})\n",
    "        \n",
    "        self.out = widgets.Output()\n",
    "        \n",
    "        # Bindings\n",
    "        self.dropdown_mode.observe(self.on_mode_change, names='value')\n",
    "        self.dropdown_algo.observe(self.update_series_list, names='value')\n",
    "        self.dropdown_dataset.observe(self.update_series_list, names='value')\n",
    "        self.dropdown_series.observe(self.on_series_change, names='value')\n",
    "        \n",
    "        display(\n",
    "            widgets.VBox([\n",
    "                self.dropdown_mode,\n",
    "                widgets.HBox([self.dropdown_algo, self.dropdown_dataset]),\n",
    "                self.dropdown_series,\n",
    "                self.out\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        # Trigger Initial Load\n",
    "        self.on_mode_change({'new': self.dropdown_mode.value})\n",
    "\n",
    "    def on_mode_change(self, change):\n",
    "        if self._updating: return\n",
    "        \n",
    "        mode = change['new']\n",
    "        if not mode: return\n",
    "        \n",
    "        self._updating = True\n",
    "        self.dropdown_algo.unobserve(self.update_series_list, names='value')\n",
    "        self.dropdown_dataset.unobserve(self.update_series_list, names='value')\n",
    "        \n",
    "        try:\n",
    "            with self.out:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Fetching Parent Runs for Mode: {mode}...\")\n",
    "                \n",
    "                key_map = {\n",
    "                    'default': ['unsupervised'],\n",
    "                    'guided': ['semi_supervised'],\n",
    "                    'grid_default': ['grid_unsupervised'],\n",
    "                    'grid_guided': ['grid_supervised']\n",
    "                }\n",
    "                keys = key_map.get(mode, [])\n",
    "                \n",
    "                raw_parents = self.analyzer.fetch_parent_runs_stats(keys)\n",
    "                self.df_parents = self.analyzer.validate_completeness(raw_parents, strategy='merge')\n",
    "                \n",
    "                if self.df_parents.empty:\n",
    "                    print(\"No valid runs found for this mode.\")\n",
    "                    self.dropdown_algo.options = []\n",
    "                    self.dropdown_dataset.options = []\n",
    "                    self.dropdown_series.options = []\n",
    "                    self.dropdown_algo.value = None\n",
    "                    self.dropdown_dataset.value = None\n",
    "                else:\n",
    "                    print(f\"Found {len(self.df_parents)} valid parent runs.\")\n",
    "                    \n",
    "                    algos = sorted(self.df_parents['algorithm'].dropna().unique())\n",
    "                    datasets = sorted(self.df_parents['dataset'].dropna().unique())\n",
    "                    \n",
    "                    self.dropdown_algo.options = algos\n",
    "                    self.dropdown_dataset.options = datasets\n",
    "                    \n",
    "                    if algos: self.dropdown_algo.value = algos[0]\n",
    "                    else: self.dropdown_algo.value = None\n",
    "                        \n",
    "                    if datasets: self.dropdown_dataset.value = datasets[0]\n",
    "                    else: self.dropdown_dataset.value = None\n",
    "                    \n",
    "        finally:\n",
    "            self.dropdown_algo.observe(self.update_series_list, names='value')\n",
    "            self.dropdown_dataset.observe(self.update_series_list, names='value')\n",
    "            self._updating = False\n",
    "        \n",
    "        self.update_series_list(None)\n",
    "\n",
    "    def update_series_list(self, change):\n",
    "        if self._updating: return\n",
    "        \n",
    "        self._updating = True\n",
    "        self.dropdown_series.unobserve(self.on_series_change, names='value')\n",
    "        \n",
    "        try:\n",
    "            algo = self.dropdown_algo.value\n",
    "            dataset = self.dropdown_dataset.value\n",
    "            \n",
    "            if self.df_parents.empty or not algo or not dataset:\n",
    "                self.dropdown_series.options = []\n",
    "                self.dropdown_series.value = None\n",
    "                return\n",
    "                \n",
    "            mask = (self.df_parents['algorithm'] == algo) & (self.df_parents['dataset'] == dataset)\n",
    "            relevant_parents = self.df_parents[mask]\n",
    "            \n",
    "            if relevant_parents.empty:\n",
    "                self.dropdown_series.options = []\n",
    "                self.dropdown_series.value = None\n",
    "                return\n",
    "                \n",
    "            with self.out:\n",
    "                print(f\"Fetching children for {len(relevant_parents)} parent(s)...\", end='\\r')\n",
    "                \n",
    "                children_metrics = self.analyzer.fetch_metrics_for_parents(\n",
    "                    relevant_parents, \n",
    "                    deduplicate_series=True\n",
    "                )\n",
    "                \n",
    "                if children_metrics.empty:\n",
    "                    print(f\"No children found for {algo} on {dataset}.\")\n",
    "                    self.dropdown_series.options = []\n",
    "                    self.dropdown_series.value = None\n",
    "                    return\n",
    "\n",
    "                self.df_children = children_metrics\n",
    "                \n",
    "                if 'trial_index' in self.df_children.columns:\n",
    "                    self.df_children['trial_index'] = pd.to_numeric(self.df_children['trial_index'], errors='coerce').fillna(-1).astype(int)\n",
    "                    self.df_children = self.df_children.sort_values('trial_index')\n",
    "                \n",
    "                options = []\n",
    "                for _, row in self.df_children.iterrows():\n",
    "                    t_idx = row.get('trial_index', '?')\n",
    "                    run_id = row['run_id']\n",
    "                    \n",
    "                    label = f\"Series {t_idx}\"\n",
    "                    if 'metrics.f1_score' in row:\n",
    "                        label += f\" | F1: {row['metrics.f1_score']:.2f}\"\n",
    "                    \n",
    "                    label += f\" ({run_id[:6]})\"\n",
    "                    options.append((label, run_id))\n",
    "                \n",
    "                self.dropdown_series.options = options\n",
    "                print(f\"Found {len(options)} series.        \")\n",
    "                \n",
    "                if options:\n",
    "                    self.dropdown_series.value = options[0][1]\n",
    "                else:\n",
    "                    self.dropdown_series.value = None\n",
    "                    \n",
    "        finally:\n",
    "            self.dropdown_series.observe(self.on_series_change, names='value')\n",
    "            self._updating = False\n",
    "        \n",
    "        if self.dropdown_series.value:\n",
    "            self.on_series_change({'new': self.dropdown_series.value})\n",
    "\n",
    "    def _get_run_artifact_path(self, run_info):\n",
    "        uri = run_info.artifact_uri\n",
    "        run_dir = None\n",
    "        \n",
    "        if uri:\n",
    "            if uri.startswith(\"file://\"):\n",
    "                p = Path(uri.replace(\"file://\", \"\"))\n",
    "                if p.exists():\n",
    "                    run_dir = p\n",
    "                elif not p.is_absolute():\n",
    "                     p_rel = PROJECT_ROOT / p\n",
    "                     if p_rel.exists():\n",
    "                         run_dir = p_rel\n",
    "        \n",
    "        if run_dir is None:\n",
    "            run_dir = self.artifact_root / run_info.experiment_id / run_info.run_id / \"artifacts\"\n",
    "            \n",
    "        return run_dir\n",
    "\n",
    "    def _load_ground_truth_via_loader(self, dataset_name, run_params):\n",
    "        try:\n",
    "            loader_params = {}\n",
    "            valid_keys = {\n",
    "                'subject_number', 'target_number', 'ts_id', 'trial', 'dimension',\n",
    "                'ts_name', 'version', 'filename', 'variables', 'subject', 'target',\n",
    "                'trial_index'\n",
    "            }\n",
    "            int_keys = ['subject_number', 'target_number', 'ts_id', 'trial', 'dimension', 'trial_index']\n",
    "            \n",
    "            for k, v in run_params.items():\n",
    "                clean_k = k.split('.')[-1]\n",
    "                if clean_k in valid_keys:\n",
    "                    if clean_k in int_keys:\n",
    "                        try: loader_params[clean_k] = int(v)\n",
    "                        except: loader_params[clean_k] = v\n",
    "                    else:\n",
    "                        loader_params[clean_k] = v\n",
    "            \n",
    "            if 'subject' in loader_params and 'subject_number' not in loader_params:\n",
    "                try: loader_params['subject_number'] = int(loader_params['subject'])\n",
    "                except: pass\n",
    "\n",
    "            data_root = PROJECT_ROOT / \"data\"\n",
    "            X, y = load_dataset(dataset_name=dataset_name, data_root=data_root, return_X_y=True, **loader_params)\n",
    "            \n",
    "            if isinstance(X, (list, tuple)):\n",
    "                if len(X) > 0:\n",
    "                    idx = 0\n",
    "                    if 'trial_index' in loader_params:\n",
    "                        idx = int(loader_params['trial_index'])\n",
    "                    elif 'trial' in loader_params and isinstance(loader_params['trial'], int):\n",
    "                        # Some loaders use 'trial' as index\n",
    "                        idx = loader_params['trial']\n",
    "                    \n",
    "                    if 0 <= idx < len(X):\n",
    "                        print(f\"Selecting series index {idx} from {len(X)} returned.\")\n",
    "                        X = X[idx]\n",
    "                        if isinstance(y, (list, tuple)) and len(y) > idx:\n",
    "                            y = y[idx]\n",
    "                        elif isinstance(y, (list, tuple)) and len(y) > 0:\n",
    "                            y = y[0]\n",
    "                    else:\n",
    "                        print(f\"Warning: Index {idx} out of bounds (0-{len(X)-1}). Using 0.\")\n",
    "                        X = X[0]\n",
    "                        if isinstance(y, (list, tuple)) and len(y) > 0:\n",
    "                            y = y[0]\n",
    "                else:\n",
    "                    return None, None\n",
    "            \n",
    "            if X is not None: X = np.array(X)\n",
    "            if y is not None: y = np.array(y)\n",
    "\n",
    "            return X, y\n",
    "        except Exception as e:\n",
    "            print(f\"Loader failed for {dataset_name}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def on_series_change(self, change):\n",
    "        if self._updating: return\n",
    "        \n",
    "        run_id = change['new']\n",
    "        if not run_id: return\n",
    "        \n",
    "        with self.out:\n",
    "            clear_output(wait=True)\n",
    "            try:\n",
    "                run = mlflow.get_run(run_id)\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching run {run_id}: {e}\")\n",
    "                return\n",
    "\n",
    "            params = run.data.params\n",
    "            dataset_name = params.get('dataset') or params.get('dataset_name')\n",
    "            if not dataset_name and self.dropdown_dataset.value:\n",
    "                dataset_name = self.dropdown_dataset.value\n",
    "            \n",
    "            algorithm_name = params.get('algorithm') or params.get('algorithm_name') or self.dropdown_algo.value\n",
    "            \n",
    "            print(f\"Run: {run_id} | Algo: {algorithm_name} | Data: {dataset_name}\")\n",
    "            \n",
    "            run_dir = self._get_run_artifact_path(run.info)\n",
    "            if run_dir.exists():\n",
    "                print(f\"Artifacts: {run_dir} -> Found\")\n",
    "            else:\n",
    "                 print(f\"Artifacts: {run_dir} -> Missing\")\n",
    "            \n",
    "            data = {}\n",
    "            if dataset_name:\n",
    "                X, y = self._load_ground_truth_via_loader(dataset_name, params)\n",
    "                if X is not None: data['X'] = X\n",
    "                if y is not None: data['y'] = y\n",
    "            \n",
    "            if run_dir.exists():\n",
    "                files = [\n",
    "                    \"segmentation.npy\", \"predicted_labels.npy\", \n",
    "                    \"predicted_change_points.npy\", \"ground_truth_change_points.npy\"\n",
    "                ]\n",
    "                for f in files:\n",
    "                    matches = list(run_dir.glob(f\"**/{f}\"))\n",
    "                    if matches:\n",
    "                        try:\n",
    "                            data[f] = np.load(matches[0], allow_pickle=True)\n",
    "                        except: pass\n",
    "            \n",
    "            if data:\n",
    "                meta = {'algorithm': algorithm_name, 'dataset': dataset_name}\n",
    "                self.plot_segmentation(data, meta)\n",
    "            else:\n",
    "                print(\"No data to display. (Loader failed and no artifacts found)\")\n",
    "\n",
    "    def _get_optimal_mapping(self, y_true, y_pred):\n",
    "        \"\"\"Aligns y_pred colors to y_true using Hungarian algorithm.\"\"\"\n",
    "        # Normalize inputs\n",
    "        y_true = np.array(y_true).flatten()\n",
    "        y_pred = np.array(y_pred).flatten()\n",
    "        \n",
    "        # Filter out NaNs if any, though usually int\n",
    "        # Limit to min length\n",
    "        l = min(len(y_true), len(y_pred))\n",
    "        y_true = y_true[:l]\n",
    "        y_pred = y_pred[:l]\n",
    "        \n",
    "        labels_true = np.unique(y_true)\n",
    "        labels_pred = np.unique(y_pred)\n",
    "        \n",
    "        # Build Confusion Matrix\n",
    "        t_map = {l: i for i, l in enumerate(labels_true)}\n",
    "        p_map = {l: i for i, l in enumerate(labels_pred)}\n",
    "        \n",
    "        C = np.zeros((len(labels_true), len(labels_pred)), dtype=int)\n",
    "        for t, p in zip(y_true, y_pred):\n",
    "            C[t_map[t], p_map[p]] += 1\n",
    "            \n",
    "        # Hungarian Algorithm\n",
    "        row_ind, col_ind = linear_sum_assignment(C, maximize=True)\n",
    "        \n",
    "        # Build Color Mappings\n",
    "        # GT Map: Just index 0..N\n",
    "        color_map_true = {l: i for i, l in enumerate(labels_true)}\n",
    "        \n",
    "        # Pred Map: Align matched, append new\n",
    "        color_map_pred = {}\n",
    "        matched_preds = set()\n",
    "        \n",
    "        for r, c in zip(row_ind, col_ind):\n",
    "            gt_label = labels_true[r]\n",
    "            pred_label = labels_pred[c]\n",
    "            color_map_pred[pred_label] = color_map_true[gt_label]\n",
    "            matched_preds.add(pred_label)\n",
    "            \n",
    "        next_color = len(labels_true)\n",
    "        for l in labels_pred:\n",
    "            if l not in matched_preds:\n",
    "                color_map_pred[l] = next_color\n",
    "                next_color += 1\n",
    "                \n",
    "        return color_map_true, color_map_pred\n",
    "\n",
    "    def plot_segmentation(self, data, meta):\n",
    "        has_X = 'X' in data\n",
    "        \n",
    "        if has_X:\n",
    "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8), sharex=True, gridspec_kw={'height_ratios': [2, 1]})\n",
    "        else:\n",
    "            fig, ax2 = plt.subplots(figsize=(15, 4))\n",
    "            ax1 = None\n",
    "\n",
    "        # --- Top Panel: Time Series ---\n",
    "        if has_X:\n",
    "            X = data['X']\n",
    "            if not isinstance(X, np.ndarray):\n",
    "                try: X = np.array(X)\n",
    "                except: pass\n",
    "\n",
    "            if isinstance(X, np.ndarray) and X.ndim > 1:\n",
    "                # Plot max 10 dims\n",
    "                for i in range(min(X.shape[1], 10)):\n",
    "                    ax1.plot(X[:, i], alpha=0.5, linewidth=1)\n",
    "            elif isinstance(X, np.ndarray):\n",
    "                ax1.plot(X, color='black', alpha=0.6)\n",
    "            \n",
    "            ax1.set_title(f\"Time Series: {meta['dataset']}\")\n",
    "            ax1.set_ylabel(\"Value\")\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # optional: GT Change Points overlays on TS\n",
    "            gt_cps = None\n",
    "            if \"ground_truth_change_points.npy\" in data:\n",
    "                gt_cps = data[\"ground_truth_change_points.npy\"]\n",
    "            elif 'y' in data:\n",
    "                 y = data['y']\n",
    "                 try: gt_cps = np.where(y[:-1] != y[1:])[0]\n",
    "                 except: pass \n",
    "            \n",
    "            if gt_cps is not None:\n",
    "                gt_cps = np.array(gt_cps).flatten()\n",
    "                for cp in gt_cps:\n",
    "                    ax1.axvline(x=cp, color='green', linestyle='--', alpha=0.5)\n",
    "\n",
    "            # optional: Predicted Change Points overlays on TS\n",
    "            pred_cps = None\n",
    "            if \"predicted_change_points.npy\" in data:\n",
    "                pred_cps = data[\"predicted_change_points.npy\"]\n",
    "            \n",
    "            if pred_cps is not None:\n",
    "                pred_cps = np.array(pred_cps).flatten()\n",
    "                for cp in pred_cps:\n",
    "                    ax1.axvline(x=cp, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "            # Legend\n",
    "            if gt_cps is not None or pred_cps is not None:\n",
    "                handles = []\n",
    "                if gt_cps is not None:\n",
    "                    handles.append(mlines.Line2D([], [], color='green', linestyle='--', label='GT CP'))\n",
    "                if pred_cps is not None:\n",
    "                    handles.append(mlines.Line2D([], [], color='red', linestyle='--', label='Pred CP'))\n",
    "                ax1.legend(handles=handles, loc='upper right')\n",
    "\n",
    "        # --- Bottom Panel: Colored Segmentation Bars ---\n",
    "        # Prepare Data\n",
    "        y_true = np.array([])\n",
    "        y_pred = np.array([])\n",
    "        \n",
    "        if 'y' in data: \n",
    "            y_true = np.array(data['y']).flatten()\n",
    "            \n",
    "        if \"predicted_labels.npy\" in data:\n",
    "            y_pred = np.array(data[\"predicted_labels.npy\"]).flatten()\n",
    "            \n",
    "        # Determine Length for visualization\n",
    "        T = 0\n",
    "        if len(y_true) > 0: T = max(T, len(y_true))\n",
    "        if len(y_pred) > 0: T = max(T, len(y_pred))\n",
    "        \n",
    "        if T == 0 and has_X: T = len(data['X'])\n",
    "        \n",
    "        if T > 0:\n",
    "            # Color Matching\n",
    "            if len(y_true) > 0 and len(y_pred) > 0:\n",
    "                cmap_t, cmap_p = self._get_optimal_mapping(y_true, y_pred)\n",
    "            else:\n",
    "                # Fallback simple maps\n",
    "                cmap_t = {l: i for i, l in enumerate(np.unique(y_true))} if len(y_true) > 0 else {}\n",
    "                cmap_p = {l: i for i, l in enumerate(np.unique(y_pred))} if len(y_pred) > 0 else {}\n",
    "\n",
    "            # Construct Image Grid (2 Rows: GT, Pred)\n",
    "            # Use float/nan for empty spaces\n",
    "            grid = np.full((2, T), np.nan)\n",
    "            \n",
    "            # Fill GT (Row 0)\n",
    "            if len(y_true) > 0:\n",
    "                indices = [cmap_t.get(val, 0) for val in y_true]\n",
    "                grid[0, :len(indices)] = indices\n",
    "                \n",
    "            # Fill Pred (Row 1)\n",
    "            if len(y_pred) > 0:\n",
    "                indices = [cmap_p.get(val, 0) for val in y_pred]\n",
    "                grid[1, :len(indices)] = indices\n",
    "            \n",
    "            # Plot\n",
    "            # Use interpolation='nearest' and aspect='auto' for distinct blocks\n",
    "            ax2.imshow(\n",
    "                grid, \n",
    "                aspect='auto', \n",
    "                interpolation='nearest', \n",
    "                cmap='tab20',\n",
    "                extent=[0, T, -0.5, 1.5] # y-range -0.5 to 1.5 allows ticks at 0 and 1\n",
    "            )\n",
    "            \n",
    "            # Formatting\n",
    "            ax2.set_yticks([1, 0])\n",
    "            ax2.set_yticklabels([\"GT states\", \"Predicted states\"])\n",
    "            \n",
    "            # Separator line\n",
    "            ax2.axhline(y=0.5, color='black', linewidth=1.5)\n",
    "            \n",
    "            ax2.set_title(f\"Segmentation: {meta['algorithm']}\")\n",
    "            ax2.set_xlabel(\"Time Step\")\n",
    "            ax2.grid(False) # Grid interferes with bars\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Start Dashboard\n",
    "dashboard = SegmentationDashboard(analyzer, ARTIFACT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fd124eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analyzing Run: ed38d4935f4346f19449319228d2fc05 (GGS | Mocap | Series 1) ---\n",
      "DB Logged Covering Score: 0.6302778974579922\n",
      "Loading artifacts from: /home/fchavell/tsseg-project/tsseg-exp/mlartifacts/10/ed38d4935f4346f19449319228d2fc05/artifacts\n",
      "\n",
      "Loaded Ground Truth CPs: [   0  572 1012 1232 1408]\n",
      "Loaded Predicted CPs:    [   0  564 1408]\n",
      "Covering legacy 0.6302778974579922\n",
      "y_true: [0, 572, 1012, 1232, 1408]\n",
      "y_pred: [0, 564, 1408]\n",
      "\n",
      "Recomputed Covering Score: 0.6302778974579922\n",
      "Difference: 0.000000\n",
      ">> MATCH: Scores are identical.\n"
     ]
    }
   ],
   "source": [
    "from tsseg.metrics.change_point_detection import Covering\n",
    "\n",
    "# Target Run Parameters\n",
    "run_id = \"ed38d4935f4346f19449319228d2fc05\"\n",
    "print(f\"--- Analyzing Run: {run_id} (GGS | Mocap | Series 1) ---\")\n",
    "\n",
    "# 1. Fetch Logged Metric from DB\n",
    "run = mlflow.get_run(run_id)\n",
    "# Note: Metric name might vary slightly depending on logging prefix (e.g. 'covering', 'metrics.covering')\n",
    "logged_score = run.data.metrics.get('covering_score')\n",
    "if logged_score is None:\n",
    "    # Try alternate keys often found in MLflow\n",
    "    logged_score = run.data.metrics.get('metrics.covering_score')\n",
    "\n",
    "print(f\"DB Logged Covering Score: {logged_score}\")\n",
    "\n",
    "# 2. Load Artifacts (True & Pred CPs)\n",
    "# We use ARTIFACT_ROOT defined in the configuration cell above\n",
    "run_artifact_dir = ARTIFACT_ROOT / run.info.experiment_id / run_id / \"artifacts\"\n",
    "print(f\"Loading artifacts from: {run_artifact_dir}\")\n",
    "\n",
    "try:\n",
    "    # Load Change Points directly\n",
    "    y_true = np.load(run_artifact_dir / \"ground_truth_change_points.npy\", allow_pickle=True).flatten()\n",
    "    y_pred = np.load(run_artifact_dir / \"predicted_change_points.npy\", allow_pickle=True).flatten()\n",
    "\n",
    "    print(f\"\\nLoaded Ground Truth CPs: {y_true}\")\n",
    "    print(f\"Loaded Predicted CPs:    {y_pred}\")\n",
    "\n",
    "    # 3. Recompute Metric explicitly using current codebase\n",
    "    # We set convert_labels_to_segments=False because we are providing CPs directly, not labels.\n",
    "    covering_metric = Covering(convert_labels_to_segments=False)\n",
    "    \n",
    "    # Compute\n",
    "    result = covering_metric.compute(y_true, y_pred)\n",
    "    recomputed_score = result['score']\n",
    "    \n",
    "    print(f\"\\nRecomputed Covering Score: {recomputed_score}\")\n",
    "    \n",
    "    if logged_score is not None:\n",
    "        diff = abs(logged_score - recomputed_score)\n",
    "        print(f\"Difference: {diff:.6f}\")\n",
    "        if diff < 1e-6:\n",
    "            print(\">> MATCH: Scores are identical.\")\n",
    "        else:\n",
    "            print(\">> DIFF: Scores differ (check metric version or boundary handling).\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Could not find artifact file. {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during calculation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb929c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Auditing Predicted Change Point Boundaries (Random Sample 1000 Runs) ---\n",
      "Total runs found: 100000\n",
      "Finished runs: 98750\n",
      "Auditing 1000 runs...\n",
      "\n",
      "--- Audit Results ---\n",
      "Runs checked: 983 (skipped 17 missing artifacts)\n",
      "Runs with '0' included: 983 (100.0%)\n",
      "Runs with 'T' included: 983 (100.0%)\n",
      "\n",
      "Runs missing boundaries: 0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "print(\"--- Auditing Predicted Change Point Boundaries (Random Sample 1000 Runs) ---\")\n",
    "\n",
    "# 1. Fetch all runs\n",
    "search_results = mlflow.search_runs(search_all_experiments=True, output_format=\"list\")\n",
    "print(f\"Total runs found: {len(search_results)}\")\n",
    "\n",
    "# Filter finished runs\n",
    "finished_runs = [r for r in search_results if r.info.status == \"FINISHED\"]\n",
    "print(f\"Finished runs: {len(finished_runs)}\")\n",
    "\n",
    "# 2. Sample 1000 runs\n",
    "sample_size = 1000\n",
    "if len(finished_runs) > sample_size:\n",
    "    sampled_runs = random.sample(finished_runs, sample_size)\n",
    "else:\n",
    "    sampled_runs = finished_runs\n",
    "print(f\"Auditing {len(sampled_runs)} runs...\")\n",
    "\n",
    "# Counters\n",
    "count_has_0 = 0\n",
    "count_has_T = 0\n",
    "count_no_pred = 0\n",
    "count_missing_artifacts = 0\n",
    "boundary_issues = []\n",
    "\n",
    "for idx, run in enumerate(sampled_runs):\n",
    "    run_id = run.info.run_id\n",
    "    exp_id = run.info.experiment_id\n",
    "    \n",
    "    # Construct artifact path\n",
    "    artifact_path = ARTIFACT_ROOT / exp_id / run_id / \"artifacts\"\n",
    "    \n",
    "    # We need both True and Pred to know what T is\n",
    "    pred_path = artifact_path / \"predicted_change_points.npy\"\n",
    "    true_path = artifact_path / \"ground_truth_change_points.npy\"\n",
    "    \n",
    "    if not pred_path.exists() or not true_path.exists():\n",
    "        count_missing_artifacts += 1\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        y_pred = np.load(pred_path, allow_pickle=True).flatten()\n",
    "        y_true = np.load(true_path, allow_pickle=True).flatten()\n",
    "        \n",
    "        # Determine T from Ground Truth (last element of y_true usually)\n",
    "        # Assuming y_true is a list of change points, the last one is the length T\n",
    "        if len(y_true) == 0:\n",
    "            continue\n",
    "            \n",
    "        T = y_true[-1] \n",
    "        \n",
    "        if len(y_pred) == 0:\n",
    "            count_no_pred += 1\n",
    "            continue\n",
    "            \n",
    "        # Check 0\n",
    "        has_0 = (0 in y_pred)\n",
    "        if has_0: count_has_0 += 1\n",
    "        \n",
    "        # Check T\n",
    "        has_T = (T in y_pred)\n",
    "        if has_T: count_has_T += 1\n",
    "        \n",
    "        if not has_0 or not has_T:\n",
    "            boundary_issues.append({\n",
    "                'run_id': run_id,\n",
    "                'algo': run.data.params.get('algorithm'),\n",
    "                'dataset': run.data.params.get('dataset'),\n",
    "                'has_0': has_0,\n",
    "                'has_T': has_T,\n",
    "                'pred': y_pred,\n",
    "                'T': T\n",
    "            })\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing run {run_id}: {e}\")\n",
    "\n",
    "print(\"\\n--- Audit Results ---\")\n",
    "print(f\"Runs checked: {len(sampled_runs) - count_missing_artifacts} (skipped {count_missing_artifacts} missing artifacts)\")\n",
    "print(f\"Runs with '0' included: {count_has_0} ({count_has_0/(len(sampled_runs)-count_missing_artifacts):.1%})\")\n",
    "print(f\"Runs with 'T' included: {count_has_T} ({count_has_T/(len(sampled_runs)-count_missing_artifacts):.1%})\")\n",
    "print(f\"\\nRuns missing boundaries: {len(boundary_issues)}\")\n",
    "\n",
    "if boundary_issues:\n",
    "    print(\"\\nSample of runs with missing boundaries:\")\n",
    "    for issue in boundary_issues[:10]:\n",
    "        print(f\"Run {issue['run_id']} ({issue['algo']}): Includes 0? {issue['has_0']}, Includes T? {issue['has_T']} (T={issue['T']}) -> Pred: {issue['pred']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08d49f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Recomputing Metrics for All Runs on TSSB ---\n",
      "Available Algorithms: ['amoc', 'binseg', 'clap', 'clasp', 'eagglo', 'espresso', 'fluss', 'hdp-hsmm-legacy', 'hidalgo', 'icid', 'kcpd', 'pelt', 'prophet', 'random', 'tglad', 'ticc', 'time2state', 'tire', 'tscp2', 'vsax', 'window']\n",
      "Available Datasets:   ['actrectut', 'has', 'mocap', 'pamap2', 'skab', 'tssb', 'usc-had', 'utsa']\n",
      "Found 330 matching runs for dataset tssb.\n",
      "Starting audit loop...\n",
      "SKIP 1b202645: Files missing at /run/user/675409/gvfs/sftp:host=cleps,user=fchavell/home/fchavell/scratch/tsseg-exp/mlartifacts/15/1b20264576a748a8864f490b0836d55e/artifacts\n",
      "      (URI: /scratch/fchavell/tsseg-exp/mlartifacts/15/1b20264576a748a8864f490b0836d55e/artifacts)\n",
      "SKIP 3c1400cd: Files missing at /run/user/675409/gvfs/sftp:host=cleps,user=fchavell/home/fchavell/scratch/tsseg-exp/mlartifacts/15/3c1400cdea0547d38be4da2063deb8b5/artifacts\n",
      "      (URI: /scratch/fchavell/tsseg-exp/mlartifacts/15/3c1400cdea0547d38be4da2063deb8b5/artifacts)\n",
      "SKIP 145d2147: Files missing at /run/user/675409/gvfs/sftp:host=cleps,user=fchavell/home/fchavell/scratch/tsseg-exp/mlartifacts/15/145d2147c65d479cb8c931b9bf72eaa1/artifacts\n",
      "      (URI: /scratch/fchavell/tsseg-exp/mlartifacts/15/145d2147c65d479cb8c931b9bf72eaa1/artifacts)\n",
      "SKIP 5000145f: Files missing at /run/user/675409/gvfs/sftp:host=cleps,user=fchavell/home/fchavell/scratch/tsseg-exp/mlartifacts/15/5000145fbc9640e9babac184aa6dfafb/artifacts\n",
      "      (URI: /scratch/fchavell/tsseg-exp/mlartifacts/15/5000145fbc9640e9babac184aa6dfafb/artifacts)\n",
      "SKIP bc91d7fd: Files missing at /run/user/675409/gvfs/sftp:host=cleps,user=fchavell/home/fchavell/scratch/tsseg-exp/mlartifacts/15/bc91d7fda29c47c5a1116a7545bff42e/artifacts\n",
      "      (URI: /scratch/fchavell/tsseg-exp/mlartifacts/15/bc91d7fda29c47c5a1116a7545bff42e/artifacts)\n",
      "No comparison results generated (all artifacts missing or errored).\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- Recomputing Metrics for All Runs on TSSB ---\")\n",
    "\n",
    "# 0. Debug: Check available values in finished_runs\n",
    "seen_algos = set()\n",
    "seen_datasets = set()\n",
    "for r in finished_runs:\n",
    "    p = r.data.params\n",
    "    a = p.get('algorithm') or p.get('algorithm_name')\n",
    "    d = p.get('dataset') or p.get('dataset_name')\n",
    "    if a: seen_algos.add(a)\n",
    "    if d: seen_datasets.add(d)\n",
    "\n",
    "print(f\"Available Algorithms: {sorted(list(seen_algos))}\")\n",
    "print(f\"Available Datasets:   {sorted(list(seen_datasets))}\")\n",
    "\n",
    "# 1. Filter runs for ANY Algo on TSSB (Robust Filtering)\n",
    "tssb_runs = []\n",
    "target_dataset = 'tssb'\n",
    "\n",
    "for r in finished_runs:\n",
    "    p = r.data.params\n",
    "    # Handle key variations\n",
    "    dataset = p.get('dataset') or p.get('dataset_name')\n",
    "    \n",
    "    if dataset:\n",
    "        if dataset.lower() == target_dataset.lower():\n",
    "            tssb_runs.append(r)\n",
    "\n",
    "print(f\"Found {len(tssb_runs)} matching runs for dataset {target_dataset}.\")\n",
    "\n",
    "# Metric setup\n",
    "# Ensure import is available if running this cell standalone (assuming tsseg installed)\n",
    "try:\n",
    "    from tsseg.metrics.change_point_detection import Covering\n",
    "except ImportError:\n",
    "    pass # Assume loaded from previous cells\n",
    "\n",
    "covering_metric = Covering(convert_labels_to_segments=False)\n",
    "comparison_results = []\n",
    "\n",
    "print(\"Starting audit loop...\")\n",
    "for idx, run in enumerate(tssb_runs):\n",
    "    run_id = run.info.run_id\n",
    "    algo_name = run.data.params.get('algorithm') or run.data.params.get('algorithm_name') or 'unknown'\n",
    "\n",
    "    # Get Logged Score\n",
    "    logged_score = run.data.metrics.get('covering_score') or run.data.metrics.get('metrics.covering_score')\n",
    "    \n",
    "    # --- Smart Artifact Resolution ---\n",
    "    run_artifact_dir = None\n",
    "    uri = run.info.artifact_uri\n",
    "    \n",
    "    # 1. Try URI if local file\n",
    "    if uri and uri.startswith(\"file://\"):\n",
    "        p = Path(uri.replace(\"file://\", \"\"))\n",
    "        if p.exists():\n",
    "            run_artifact_dir = p\n",
    "        elif not p.is_absolute():\n",
    "             # Try relative to project root\n",
    "             p_rel = PROJECT_ROOT / p\n",
    "             if p_rel.exists():\n",
    "                 run_artifact_dir = p_rel\n",
    "    \n",
    "    # 2. Fallback to constructed path\n",
    "    if run_artifact_dir is None:\n",
    "        run_artifact_dir = ARTIFACT_ROOT / run.info.experiment_id / run_id / \"artifacts\"\n",
    "        \n",
    "    pred_path = run_artifact_dir / \"predicted_change_points.npy\"\n",
    "    true_path = run_artifact_dir / \"ground_truth_change_points.npy\"\n",
    "    \n",
    "    if not pred_path.exists() or not true_path.exists():\n",
    "        if idx < 5: # Debug first 5 failures\n",
    "            print(f\"SKIP {run_id[:8]}: Files missing at {run_artifact_dir}\")\n",
    "            print(f\"      (URI: {uri})\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        y_pred = np.load(pred_path, allow_pickle=True).flatten()\n",
    "        y_true = np.load(true_path, allow_pickle=True).flatten()\n",
    "        \n",
    "        # Recompute\n",
    "        result = covering_metric.compute(y_true, y_pred)\n",
    "        recomputed_score = result['score']\n",
    "        \n",
    "        # Handle logged_score being None\n",
    "        l_score_val = logged_score if logged_score is not None else -1.0\n",
    "        \n",
    "        diff = abs(l_score_val - recomputed_score)\n",
    "        is_match = (diff < 1e-6) if logged_score is not None else False\n",
    "        \n",
    "        comparison_results.append({\n",
    "            'run_id': run_id,\n",
    "            'algo': algo_name,\n",
    "            'logged': logged_score,\n",
    "            'recomputed': recomputed_score,\n",
    "            'diff': diff,\n",
    "            'match': is_match\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error on run {run_id}: {e}\")\n",
    "\n",
    "# Summary\n",
    "if comparison_results:\n",
    "    df_comp = pd.DataFrame(comparison_results)\n",
    "    print(f\"\\nStats for {len(df_comp)} successful recalculations:\")\n",
    "    match_rate = df_comp['match'].mean()\n",
    "    print(f\"Match Rate: {match_rate:.1%}\")\n",
    "    metric_diffs = df_comp[~df_comp['match']]\n",
    "    if not metric_diffs.empty:\n",
    "        print(f\"Average Diff (mismatches): {metric_diffs['diff'].mean():.6f}\")\n",
    "    \n",
    "    print(\"\\nTop differences:\")\n",
    "    print(df_comp.sort_values('diff', ascending=False).head(10)[['run_id', 'algo', 'logged', 'recomputed', 'diff']])\n",
    "    \n",
    "    # Algo breakdown\n",
    "    print(\"\\nMismatch Rate by Algorithm:\")\n",
    "    print(df_comp.groupby('algo')['match'].mean().sort_values())\n",
    "else:\n",
    "    print(\"No comparison results generated (all artifacts missing or errored).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "721fb546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DIAGNOSTIC SQL Search: Algo ~= 'clasp', Dataset ~= 'tssb' (Exp ID: 10) ---\n",
      "Found 1 raw runs matching algo/dataset in Exp 10.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>status</th>\n",
       "      <th>trial_index</th>\n",
       "      <th>parent_run_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def3aeb1cfc84943be94df6efb4490e2</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.629993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             run_id    status trial_index parent_run_id  \\\n",
       "0  def3aeb1cfc84943be94df6efb4490e2  FINISHED        None          None   \n",
       "\n",
       "      score  \n",
       "0  0.629993  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Structure Analysis:\n",
      "- Runs WITHOUT parent_run_id (potential parents): 1\n",
      "- Runs WITH parent_run_id (children): 0\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "target_algo_search = \"clasp\"    # e.g., \"clasp\", \"pelt\"\n",
    "target_dataset_search = \"tssb\"  # e.g., \"tssb\", \"has\"\n",
    "\n",
    "print(f\"--- DIAGNOSTIC SQL Search: Algo ~= '{target_algo_search}', Dataset ~= '{target_dataset_search}' (Exp ID: 10) ---\")\n",
    "\n",
    "# Access variables defined in previous cells\n",
    "db_path = str(MLFLOW_DB_PATH)\n",
    "\n",
    "# SQL Query: Broad search to see what runs exist and what tags/params they have\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    r.run_uuid as run_id, \n",
    "    r.end_time,\n",
    "    r.status,\n",
    "    p_algo.value as algorithm,\n",
    "    p_data.value as dataset,\n",
    "    -- Check if 'trial_index' param exists\n",
    "    (SELECT value FROM params WHERE run_uuid = r.run_uuid AND key = 'trial_index') as trial_index,\n",
    "    -- Check if 'mlflow.parentRunId' tag exists\n",
    "    (SELECT value FROM tags WHERE run_uuid = r.run_uuid AND key = 'mlflow.parentRunId') as parent_run_id,\n",
    "     -- Check covering score\n",
    "    (SELECT value FROM metrics WHERE run_uuid = r.run_uuid AND key IN ('covering_score', 'metrics.covering_score') ORDER BY value DESC LIMIT 1) as score\n",
    "FROM runs r\n",
    "JOIN params p_algo ON r.run_uuid = p_algo.run_uuid \n",
    "JOIN params p_data ON r.run_uuid = p_data.run_uuid \n",
    "WHERE r.experiment_id = '10'\n",
    "  AND p_algo.key IN ('algorithm', 'algorithm_name') \n",
    "  AND p_algo.value LIKE '%{target_algo_search}%'\n",
    "  AND p_data.key IN ('dataset', 'dataset_name') \n",
    "  AND p_data.value LIKE '%{target_dataset_search}%'\n",
    "ORDER BY r.end_time DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        df_found = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    if not df_found.empty:\n",
    "        # Convert ms timestamp to date\n",
    "        if 'end_time' in df_found.columns:\n",
    "            df_found['date'] = pd.to_datetime(df_found['end_time'], unit='ms')\n",
    "            \n",
    "        print(f\"Found {len(df_found)} raw runs matching algo/dataset in Exp 10.\")\n",
    "        display(df_found[['run_id', 'status', 'trial_index', 'parent_run_id', 'score']].head(20))\n",
    "        \n",
    "        # Check distribution\n",
    "        n_parents = df_found['parent_run_id'].isna().sum()\n",
    "        n_children = df_found['parent_run_id'].notna().sum()\n",
    "        print(f\"\\nStructure Analysis:\")\n",
    "        print(f\"- Runs WITHOUT parent_run_id (potential parents): {n_parents}\")\n",
    "        print(f\"- Runs WITH parent_run_id (children): {n_children}\")\n",
    "    else:\n",
    "        print(\"No runs found AT ALL for this Algo/Dataset in Exp 10.\")\n",
    "        print(\"Check if Experiment ID is correct (Unsupervised usually Exp 1?) or if Algo/Dataset names match.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"SQL Error: {e}\")\n",
    "    print(f\"Query was:\\n{query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9501fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspection de la base : /home/fchavell/tsseg-project/tsseg-exp/results/mlflow_snapshot.db\n",
      "\n",
      "--- Tables disponibles ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>experiments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alembic_version</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>experiment_tags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>registered_models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>runs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>registered_model_tags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>model_versions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>latest_metrics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>metrics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>registered_model_aliases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>inputs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>input_tags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>params</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>trace_info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>trace_tags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>trace_request_metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>datasets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>logged_models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>logged_model_metrics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>logged_model_params</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>logged_model_tags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>model_version_tags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>assessments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>spans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>entity_associations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>webhooks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>webhook_events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>scorers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>scorer_versions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>evaluation_datasets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>evaluation_dataset_tags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>evaluation_dataset_records</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>jobs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>secrets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>endpoints</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>model_definitions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>endpoint_model_mappings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>endpoint_bindings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>endpoint_tags</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name\n",
       "0                  experiments\n",
       "1              alembic_version\n",
       "2              experiment_tags\n",
       "3            registered_models\n",
       "4                         runs\n",
       "5        registered_model_tags\n",
       "6               model_versions\n",
       "7               latest_metrics\n",
       "8                      metrics\n",
       "9     registered_model_aliases\n",
       "10                      inputs\n",
       "11                  input_tags\n",
       "12                      params\n",
       "13                  trace_info\n",
       "14                  trace_tags\n",
       "15      trace_request_metadata\n",
       "16                        tags\n",
       "17                    datasets\n",
       "18               logged_models\n",
       "19        logged_model_metrics\n",
       "20         logged_model_params\n",
       "21           logged_model_tags\n",
       "22          model_version_tags\n",
       "23                 assessments\n",
       "24                       spans\n",
       "25         entity_associations\n",
       "26                    webhooks\n",
       "27              webhook_events\n",
       "28                     scorers\n",
       "29             scorer_versions\n",
       "30         evaluation_datasets\n",
       "31     evaluation_dataset_tags\n",
       "32  evaluation_dataset_records\n",
       "33                        jobs\n",
       "34                     secrets\n",
       "35                   endpoints\n",
       "36           model_definitions\n",
       "37     endpoint_model_mappings\n",
       "38           endpoint_bindings\n",
       "39               endpoint_tags"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SchÃ©ma de la table 'runs' ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>notnull</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>run_uuid</td>\n",
       "      <td>VARCHAR(32)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>name</td>\n",
       "      <td>VARCHAR(250)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>source_type</td>\n",
       "      <td>VARCHAR(20)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>source_name</td>\n",
       "      <td>VARCHAR(500)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entry_point_name</td>\n",
       "      <td>VARCHAR(50)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>user_id</td>\n",
       "      <td>VARCHAR(256)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>status</td>\n",
       "      <td>VARCHAR(9)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>start_time</td>\n",
       "      <td>BIGINT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>end_time</td>\n",
       "      <td>BIGINT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>source_version</td>\n",
       "      <td>VARCHAR(50)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lifecycle_stage</td>\n",
       "      <td>VARCHAR(20)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>artifact_uri</td>\n",
       "      <td>VARCHAR(200)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>experiment_id</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>deleted_time</td>\n",
       "      <td>BIGINT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name          type  notnull\n",
       "0           run_uuid   VARCHAR(32)        1\n",
       "1               name  VARCHAR(250)        0\n",
       "2        source_type   VARCHAR(20)        0\n",
       "3        source_name  VARCHAR(500)        0\n",
       "4   entry_point_name   VARCHAR(50)        0\n",
       "5            user_id  VARCHAR(256)        0\n",
       "6             status    VARCHAR(9)        0\n",
       "7         start_time        BIGINT        0\n",
       "8           end_time        BIGINT        0\n",
       "9     source_version   VARCHAR(50)        0\n",
       "10   lifecycle_stage   VARCHAR(20)        0\n",
       "11      artifact_uri  VARCHAR(200)        0\n",
       "12     experiment_id       INTEGER        0\n",
       "13      deleted_time        BIGINT        0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Exemple de paramÃ¨tres (clÃ©s utilisÃ©es) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>algo_K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>algo_K_states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>algo__target_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>algo_a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>algo_alpha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>algo_alphabet_size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>algo_axis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>algo_b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>algo_batch_size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>algo_beta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  key\n",
       "0              algo_K\n",
       "1       algo_K_states\n",
       "2       algo__target_\n",
       "3              algo_a\n",
       "4          algo_alpha\n",
       "5  algo_alphabet_size\n",
       "6           algo_axis\n",
       "7              algo_b\n",
       "8     algo_batch_size\n",
       "9           algo_beta"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Utilise le chemin dÃ©fini plus haut dans votre notebook\n",
    "print(f\"Inspection de la base : {MLFLOW_DB_PATH}\")\n",
    "\n",
    "try:\n",
    "    with sqlite3.connect(MLFLOW_DB_PATH) as conn:\n",
    "        # 1. Lister toutes les tables\n",
    "        tables = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table';\", conn)\n",
    "        print(\"\\n--- Tables disponibles ---\")\n",
    "        display(tables)\n",
    "        \n",
    "        # 2. Voir les colonnes de la table 'runs' (principale)\n",
    "        if 'runs' in tables['name'].values:\n",
    "            schema = pd.read_sql_query(\"PRAGMA table_info(runs);\", conn)\n",
    "            print(\"\\n--- SchÃ©ma de la table 'runs' ---\")\n",
    "            display(schema[['name', 'type', 'notnull']]) \n",
    "\n",
    "        # 3. Voir un Ã©chantillon des paramÃ¨tres (params) pour comprendre les clÃ©s\n",
    "        if 'params' in tables['name'].values:\n",
    "            print(\"\\n--- Exemple de paramÃ¨tres (clÃ©s utilisÃ©es) ---\")\n",
    "            display(pd.read_sql_query(\"SELECT DISTINCT key FROM params LIMIT 10\", conn))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erreur d'accÃ¨s Ã  la DB: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsseg-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
