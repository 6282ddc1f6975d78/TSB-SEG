% VLDB template version of 2020-08-03 enhances the ACM template, version 1.7.0:
% https://www.acm.org/publications/proceedings-template
% The ACM Latex guide provides further information about the ACM template

\pdfminorversion=7
\begin{filecontents*}[overwrite]{\jobname.xmpdata}
    \Title{Time Series Segmentation and State Detection: A Systematic Benchmark}
    \Author{Félix Chavelli, Arik Ermshaus, Paul Boniol, Patrick Sch\"afer, John Paparrizos}
    \Language{en-GB}
\end{filecontents*}


\documentclass[sigconf, nonacm, pdfa]{acmart}

\usepackage{colorprofiles}
\usepackage[a-2b,mathxmp]{pdfx}[2018/12/22]
\hypersetup{pdfstartview=}

% \usepackage[a-2b]{pdfx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{balance} 
\usepackage{needspace} 
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{diagbox}
\usepackage{tabularx}
\usepackage{verbatim}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{pifont}

\usepackage[colorinlistoftodos]{todonotes}
\setlength {\marginparwidth }{2cm}
\newtheorem{definition}{Definition}

\newcommand{\cmark}{\ding{51}}   % ✓
\newcommand{\xmark}{\ding{55}}   % ✗ 
\newcommand{\bluecmark}{{\color{blue}\cmark}}

%% The following content must be adapted for the final version
% paper-specific
\newcommand\vldbdoi{}
\newcommand\vldbpages{}
% issue-specific
\newcommand\vldbvolume{}
\newcommand\vldbissue{}
\newcommand\vldbyear{}
% should be fine as it is
\newcommand\vldbauthors{\authors}
\newcommand\vldbtitle{\shorttitle} 
% leave empty if no availability url should be set
\newcommand\vldbavailabilityurl{https://github.com/fchavelli/tsseg-exp}
% whether page numbers should be shown or not, use 'plain' for review versions, 'empty' for camera ready
\newcommand\vldbpagestyle{empty} 

\begin{document}
\title{Time Series Segmentation and State Detection: A Systematic Benchmark}

%%
%% The "author" command and its associated commands are used to define the authors and their affiliations

\author{Félix Chavelli}
\affiliation{%
  \institution{Inria, ENS, CNRS, PSL}
  \city{Paris}
  \country{France}
}
\email{felix.chavelli@inria.fr}

\author{Arik Ermshaus}
\affiliation{%
  \institution{Humboldt-Universit\"at zu Berlin}
  \city{Berlin}
  \country{Germany}
}
\email{ermshaua@informatik.hu-berlin.de}

\author{Fan Yang}
\affiliation{%
  \institution{The Ohio State University}
  \city{Columbus, Ohio}
  \country{USA}
}
\email{yang.7007@osu.edu}

\author{Paul Boniol}
\affiliation{%
  \institution{Inria, ENS, CNRS, PSL}
  \city{Paris}
  \country{France}
}
\email{paul.boniol@inria.fr}

\author{Patrick Sch\"afer}
\affiliation{%
  \institution{Humboldt-Universit\"at zu Berlin}
  \city{Berlin}
  \country{Germany}
}
\email{patrick.schaefer@hu-berlin.de}

\author{John Paparrizos}
\affiliation{%
  \institution{The Ohio State University and Aristotle University of Thessaloniki}
  \city{Columbus, Ohio}
  \country{USA}
}
\email{john@paparrizos.org}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.

\begin{abstract}
The ever-growing volume of sensor data from machines, smart devices, and the environment has led to an abundance of high-resolution, largely unannotated time series (TS). These temporally ordered measurements capture properties of underlying physical, human, industrial, and natural processes, which can often be described as abstract systems evolving through multiple latent states. Shifts in such processes, caused by external events or internal state transitions, manifest as changes in the distribution or shape of the recorded signals. The unsupervised localization of these transitions is the central objective of time series segmentation (TSS), while the subsequent identification of states from segments constitutes the task of time series state detection (TSSD). In combination, both retrospectively partition a TS into labelled segments that reveal the underlying data-generating process without relying on supervision. Previous studies have evaluated TSS or TSSD in isolation and have considered only a limited number of algorithms, often restricted to either univariate or multivariate TS. In this benchmark, we evaluate $27$ available TSS and TSSD algorithms on $472$ univariate and multivariate TS from multiple domains, including IoT, human activity, and medical condition monitoring. We analyse whether an overall best-performing algorithm exists across domains, examine the relationship between TSS and TSSD performance, investigate the impact of parameter choices, and assess the scalability of the evaluated approaches. In addition, we publish all source code and experimental results to foster systematic and reproducible future comparisons.
\end{abstract}


\maketitle

% The ever-growing amount of sensor data from machines, smart devices, and the environment leads to an abundance of high-resolution, unannotated time series (TS). These recordings encode recognizable properties of latent states and transitions from physical phenomena that can be modelled as abstract processes. The unsupervised localization and identification of these states and their transitions is the task of time series state detection (TSSD).

% Ubiquitous sensors today emit high frequency streams of numerical measurements that reflect properties of human, animal, industrial, commercial, and natural processes. Shifts in such processes, e.g. caused by external events or internal state changes, manifest as changes in the recorded signals.

% The study of natural and human-made processes often results in long sequences of temporally-ordered values, aka time series (TS). Such processes often consist of multiple states, e.g. operating modes of a machine, such that state changes in the observed processes result in changes in the distribution of shape of the measured values. Time series segmentation (TSS) tries to find such changes in TS post-hoc to deduce changes in the data-generating process. TSS is typically approached as an unsupervised learning problem aiming at the identification of segments distinguishable by some statistical property. Current algorithms for TSS require domain-dependent hyper-parameters to be set by the user, make assumptions about the TS value distribution or the types of detectable changes which limits their applicability. Common hyper-parameters are the measure of segment homogeneity and the number of change points, which are particularly hard to tune for each data set.

%%% do not modify the following VLDB block %%
%%% VLDB block start %%%
\pagestyle{\vldbpagestyle}
\begingroup\small\noindent\raggedright\textbf{PVLDB Reference Format:}\\
\vldbauthors. \vldbtitle. PVLDB, \vldbvolume(\vldbissue): \vldbpages, \vldbyear.\\
\href{https://doi.org/\vldbdoi}{doi:\vldbdoi}
\endgroup
\begingroup
\renewcommand\thefootnote{}\footnote{\noindent
This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit \url{https://creativecommons.org/licenses/by-nc-nd/4.0/} to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing \href{mailto:info@vldb.org}{info@vldb.org}. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. \\
\raggedright Proceedings of the VLDB Endowment, Vol. \vldbvolume, No. \vldbissue\ %
ISSN 2150-8097. \\
\href{https://doi.org/\vldbdoi}{doi:\vldbdoi} \\
}\addtocounter{footnote}{-1}\endgroup
%%% VLDB block end %%%

%%% do not modify the following VLDB block %%
%%% VLDB block start %%%
\ifdefempty{\vldbavailabilityurl}{}{
\vspace{.3cm}
\begingroup\small\noindent\raggedright\textbf{PVLDB Artifact Availability:}\\
The source code, data, and/or other artifacts have been made available at \url{\vldbavailabilityurl}.
\endgroup
}
%%% VLDB block end %%%

\sloppy

\section{Introduction} \label{sec:introduction}

% For material, see introductions in:
% https://link.springer.com/article/10.1007/s10618-023-00923-x
% https://www.vldb.org/pvldb/vol17/p1953-ermshaus.pdf
% https://arxiv.org/pdf/2504.01783

% The study and analysis of long-running biological, human-controlled, or physical processes, such as human activities or industrial manufacturing, is of great interest in their respective domains. The field of human activity recognition, for instance, aims to detect falls in the elderly~\cite{Yin2008Sensor}. To achieve this, human motions are conceptualized as an abstract process of distinct activities that transition from one to another. Data acquisition and analysis workflows are employed to detect activities from mobile sensors and report falls. Similarly, in industrial manufacturing, CNC machines are monitored by pre-installed IoT devices to detect tool wear~\cite{Tran2022Machine}. The condition of a tool can be abstracted as either normal or faulty, and wear detection is realized by recognising the transitions in-between. From a computational perspective, such phenomena can be modelled as abstract processes with discrete states and pairwise transitions between them. The core property of these processes is that the states are distinct and can be observed and measured over time by instrumentation. 

The proliferation of sensing and data acquisition technologies has enabled the large-scale collection of observations from biological, human-operated, and physical processes—ranging from human activities to industrial manufacturing workflows. These measurements, typically captured as real-valued observations over time, are also known as \emph{time series} (TS) in academic and industrial domains \cite{Lara2013Survey,Kanawaday2017Machine,Woollam2022SeisBench,Ermshaus2023ClaSP,paparrizos2016screening}. Analyzing this abundance of time series data plays a critical role for knowledge discovery and operational decision-making in practice, stimulating growing interest in numerous downstream analytical tasks, including but not limited to time series classification~\cite{boss2015,Sch_fer_2017,hivecote,middlehurst2024bake}, anomaly detection~\cite{paparrizos2022tsb, liu2024elephant,senin2014grammarviz,paparrizos2022vus}, indexing~\cite{lin2003symbolic,shieh2008isax,SFA2012}, motif discovery~\cite{lin2003symbolic,yuan2012visual,feremans2025motiplus}, and forecasting \cite{Yuqietal-2023-PatchTST,ansari2024chronos,liu2024timemmd,liu-etal-2024-lstprompt,pmlr-v235-liu24ae,liu2025evaluating,zhao2023performative}.

% Recent advances in the digitalization of measurement devices have greatly facilitated the acquisition of biological and physical process data. Sensors, such as those in smartphones, industrial machinery, or earth observation  stations, continuously record high-frequency real-valued observational data, termed \emph{time series} (TS)~\cite{Lara2013Survey,Kanawaday2017Machine,Woollam2022SeisBench}. State changes in the observed processes lead to variations in the recorded signals, which form the basis for analysis. Sensor data capture the unique characteristics of states as subsequences, distinguishable by either their shape or statistical properties. These distinct parts of TS are commonly called \emph{segments}, and each contains the recognizable properties of the observed process state. The transitions between segments, also called \emph{change points} (CPs), mark the time points in a recording, at which the observed process changes state. The identification of segments, CPs, and state labels forms the foundation for activity recognition~\cite{Ermshaus2023Human}, health assessment~\cite{Kemp2000Analysis}, or condition monitoring in IoT~\cite{Tran2022Machine}. 

Among these tasks, understanding changes in the underlying process, manifested as variations in the recorded signals, forms the foundation for many downstream analyses. In human activity recognition, for example, where human motion is typically represented as a sequence of discrete activities, systems learn to infer the transitions between these activities to detect high-risk events such as falls among elderly individuals~\cite{Yin2008Sensor}. Similarly, in industrial manufacturing, CNC machines are equipped with IoT-based monitoring systems to assess tool wear~\cite{Tran2022Machine}. The operational condition of a tool can be abstracted as a discrete state (e.g., normal or faulty), and wear detection corresponds to identifying transitions between these states. In both cases, the process can be modelled as a sequence of distinguishable states and the transitions connecting them, with these transitions being reflected in the continuous measurements collected over time. Sensor data often encode the characteristics of each process state as subsequences that are distinguishable by their shape or statistical properties. These subsequences are referred to as \emph{segments}, each capturing the recognizable signature of a particular process state. The boundaries between segments, known as \emph{change points} (CPs), mark the time instants at which the process transitions from one state to another. Identifying segments, CPs, and their associated state labels is fundamental to a broad range of applications, including activity recognition~\cite{Ermshaus2023Human}, health assessment~\cite{Kemp2000Analysis}, and condition monitoring in IoT systems~\cite{Tran2022Machine}.

% Over the past two decades, the decreasing costs of sensors and the growing digitalization of industry, science, and society has led to an enormous increase in applications that analyse streams of sensor recordings. For example, modern smartphones contain inertial measurement units (IMUs) with triaxial accelerometers, gyroscopes, and magnetometers that can track human activities~\cite{Baos2015Design}. Seismology relies on globally distributed stations to provide high-resolution waveform recordings used for earthquake detection and early warning~\cite{Woollam2022SeisBench}. In cardiology, electrocardiographs (ECG) capture heart beats from subjects over long periods of time to obtain insights into cardiac dynamics such as arrhythmias~\cite{Moody2001MITBIH}. Regardless of the domain, the underlying sensors emit continuous sequences of real-valued measurements at a given frequency, called sensor data (series) or \emph{time series} (TS). The literature offers a rich selection of technologies to store, manage, analyse, visualize and search in collections of TS~\cite{Liakos2022Chimp,Adams2020Monarch, Echihabi2022Hercules,Zhang2022PARROT,Wen2019RobustSTL,Yeh2016Matrix}. Common basic operations are the detection of unusual stretches called \emph{anomalies}~\cite{Paparrizos2022TSBUAD}, of repetitive structures called \emph{motifs}~\cite{Schfer2022Motiflets}, and of homogeneous subsequences called \emph{segments}~\cite{Gharghabi2018Domain}.

% Recent years brought an explosion in applications for low-cost high resolution sensors, for instance in mobile devices, manufacturing monitoring, or environmental and medical surveillance~\cite{Carvalho2019Survey}. These sensors produce large amounts of unlabeled temporally-ordered, real-valued sequences, also referred to as data series or \emph{time series (TS)}. Such signals capture the inherent statistical properties and temporal patterns of processes. Abrupt shifts or \emph{change points (CPs)} of these properties in TS indicate state transitions in the data-generating process. In human activity analysis (HAR), for instance, sensor data from accelerometers captures human behavior. Shifts in rotational deflections indicate changes in activity types that can be used to gather insight on health status or provide activity-aware services \cite{Feuz2015Automated}. Another example is medical condition monitoring, where a patient's physiological variables like heart rate or electroencephalogram (EEG) are continuously monitored and analyzed to detect trends and anomalies. Clinicians use change points in the corresponding signals to identify epilepsy or sleep problems~\cite{Malladi2013Online}. The semantic segmentation of medical images can further detect cardiac substructures that may reveal cardiovascular conditions, or locate the position and shape of constituent bones from which patient age can be estimated~\cite{Janik2021Interpretability,Davis2012Segmentation}. In predictive maintenance applications, sensors monitor industrial machinery and may detect their malfunctioning as changes in the measurements. The early identification of wear and tear in sensor recordings is crucial to ensure machine availability~\cite{Zenisek2019Machine}. The importance of these applications leads to an increasing interest in the problem of change point detection (CPD)~\cite{Truong2020Selective} and time series segmentation (TSS)~\cite{Aminikhanghahi2017Survey}. These two problems are complementary: While CPD aims at finding the exact positions of change points in a TS, TSS aims at partitioning a TS into disjoint segments corresponding to states of the data-generating process. Solving CPD thus naturally leads to a TSS method and vice versa, as change points exactly depict the indices of a TS where segments change. 

\begin{table}[t]
    \centering
    \renewcommand{\arraystretch}{1.15}
    \setlength{\tabcolsep}{6pt}
    \caption{Comparison among our and other existing TSS and TSSD benchmarks.\label{tab:benchmark_comparison}}
    \begin{tabular}{l|ccc|cc|cc}
    \toprule
    \textbf{Benchmark} &
    \multicolumn{3}{c|}{\textbf{Dataset}} &
    \multicolumn{2}{c|}{\textbf{Task}} &
    \multicolumn{2}{c}{\textbf{Evaluation}} \\
    \cmidrule(lr){2-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}
    & \# TS & U. & M. & TSS & TSSD & \# Alg. & \# Me. \\
    \midrule
    UTSA~\cite{Gharghabi2018Domain} & 32  & \cmark & \xmark & \cmark & \xmark & 3 & 1 \\
    TCPD~\cite{Burg2020Evaluation} & 42 & \cmark & \cmark & \cmark & \xmark & 14 & 2 \\
    SKAB~\cite{Katser2020SKAB} & 34  & \xmark & \cmark & \cmark & \xmark & 10 & 1 \\
    TSSB~\cite{Ermshaus2023ClaSP} & 75  & \cmark & \xmark & \cmark & \xmark & 7 & 2 \\
    HAS~\cite{Ermshaus2023Human} & 250  & \xmark & \cmark & \cmark & \xmark & 13 & 1 \\
    \midrule
    \textbf{Ours} & \textbf{472} & \bluecmark & \bluecmark & \bluecmark & \bluecmark & \textbf{26} & 2 \\
    \bottomrule
    \end{tabular}
\end{table}

The broad importance of these applications has driven increasing interest in \emph{change point detection} (CPD)~\cite{Truong2020Selective} and \emph{time series segmentation} (TSS)~\cite{Aminikhanghahi2017Survey}. These two tasks are closely related: CPD seeks to identify the precise temporal indices at which changes occur, whereas TSS partitions a TS into disjoint segments corresponding to distinct process states. Each segment boundary corresponds to a change point, making CPD and TSS naturally complementary. In addition, \emph{time series state detection} (TSSD) extends segmentation by assigning discrete state labels to segments, thereby grouping temporally disjoint but structurally similar regions of a TS. This enables the reconstruction of the latent state sequence governing the data-generating process and supports higher-level process analyses.

Throughout the last years, the literature has produced a large number of algorithms for TSS (or CPD) and TSSD. See~\cite{Truong2020Selective,Wang2024Unsupervised} for overviews of existing approaches. In addition, several TS benchmarks have been released that include experimental evaluations comparing between 3 and 13 TSS algorithms on 32 to 250 univariate or multivariate TS, using one or two evaluation measures. Table~\ref{tab:benchmark_comparison} provides an overview of these benchmarks. For TSSD, in contrast, no dedicated benchmark has been released so far. Recent contributions have extended existing TSS benchmarks to evaluate TSSD methods~\cite{Wang2023Time2State,Ermshaus2025CLaP}. However, to date, there has been no large-scale, integrated benchmark study that jointly compares TSS and TSSD approaches with respect to performance, transferability between both problems, parameter choices, and scalability.

In this paper, we present a systematic benchmark for TSS and TSSD that evaluates $20$ TSS and $6$ TSSD algorithms on $472$ univariate and multivariate TS from $7$ data collections, covering cross-domain use cases and using $2$ evaluation measures. To the best of our knowledge, this is the first study that integrates data sets from various domains, with differing sizes and dimensionalities, as well as algorithms from both TSS and TSSD, within a joint evaluation.

In total, we address four research questions:

\begin{enumerate}
    \item[RQ1:] Do we have a “one size fits all” method? % todo: add details
    \item[RQ2:] Is TSS performance a good proxy for TSSD accuracy? % todo: add details
    \item[RQ3:] How much does parameter tuning improve performance results? % todo: add details
    \item[RQ4:] How scalable are current TSS and TSSD methods? % todo: add details
\end{enumerate}

To foster the reproducibility of our findings and enable future systematic comparisons, we created two GitHub repositories~\cite{todo,tdo} that contain (a) the source codes of all evaluated methods, and (b) our evaluation framework, including Jupyter notebooks, raw measurements, scores, and visualizations.

The remainder of this paper is organized as follows. Section~\ref{sec:definitions} introduces the necessary definitions and background. Sections~\ref{sec:tss} to~\ref{sec:differences} present the TSS and TSSD research problems and discuss differences between existing approaches in detail. Section~\ref{sec:benchmark} describes the benchmark setup. Section~\ref{sec:experiments} reports experimental results and key findings, and Section~\ref{sec:conclusion} concludes the paper.


\section{Definitions} \label{sec:definitions}

% Definitions for TS, subsequences, CPs, segmentations: https://link.springer.com/article/10.1007/s10618-023-00923-x
% Definitions for processes, states, state sequences: https://arxiv.org/pdf/2504.01783

% In this section, we formally define the concepts of abstract processes, time series, subsequences, and state sequences.
In this section, we formally define the necessary concepts for time-series change point detection, segmentation, and state detection, including abstract processes, time series, subsequences, and state sequences.

\begin{definition}
An \emph{abstract process} $P = (S,R)$ consists of one or more discrete and distinct states $s_1, ..., s_l \in S$ that are pairwise separated by transitions $(s_i,s_j) \in R \subseteq  S \times S$, with $s_i \neq s_j$.
\end{definition}

% Following the definition of Wang et al.~\cite{Wang2024Unsupervised}, states refer to distinct phases of real-world processes observable through sensor measurements. We assume each state has a stationary, recognizable property that persists over time and makes it distinguishable from other states. Examples include human activities or machine conditions. This definition excludes processes with homonym states, that emit indistinguishable signals, and synonym states, which cannot be recognized by observation. It also excludes states that have drifting semantics over time.

Following the definition of Wang et al.~\cite{Wang2024Unsupervised}, states denote distinct phases of real-world processes that can be observed and distinguished through data measurements such as sensor signals and industrial manufacturing workflow. By assumption, each state exhibits stationary and unique properties that remain consistent over time, allowing it to be distinguished from other states; examples include human activities or machine operating conditions as illustrated in Section~\ref{sec:introduction}. Notably, this definition excludes ``homonym states,'' which emit indistinguishable signals and lead to ambiguity in partitioning different segments. Additionally, states whose semantics drift over time are also excluded to ease discussion.


% Transitions are, by definition, changes between states and constitute the links among them in processes. We assume that a transition leads to a change in the observations of a process. Gradual changes, such as trends, can be modelled as separate states. We make no assumptions about transition causes, i.e., whether they occur randomly or systematically.

Correspondingly, transitions denote the boundaries or changes between states and serve as the links connecting adjacent states. By definition, each transition must induce a measurable change in the observed data in order to be detectable. Gradual shifts—such as long-term trends—may be modeled as separate states if they exhibit distinguishable observational characteristics. We do not impose any assumptions on the underlying causes of transitions, whether they occur randomly or follow systematic patterns.

% For analysis, we consider measurements emitted by one or multiple sensors observing outcomes or byproducts of a process. Human activity, for instance, can be tracked by inertial measurement units (IMUs) in smartphones~\cite{Lara2013Survey} and industrial machinery can be monitored with IoT devices~\cite{Wang2020Apache}, which results in temporal data.

We start from measurements produced by one or more sensors that capture observable outcomes or byproducts of the process. For example, human activities can be monitored by inertial measurement units (IMUs) embedded in smartphones~\cite{Lara2013Survey}, whereas industrial machinery can be measured using IoT-based sensing systems~\cite{Wang2020Apache}, both producing temporally ordered data.

\begin{definition}
A \emph{time series} (TS) $T$ is an ordered sequence of $n \times d \in \mathbb{N}^2$ dimensional vectors $T=(\vec{t}_{1},\dots,\vec{t}_{n}) \in \mathbb{R}^{n \times d}$ that simultaneously measures $d$ observable outputs of a process $P$.
\end{definition}

% Each $t_i$ has $d$ dimensions, or channels, one for each sensor. Its values, also called data points or measurements, are equi-distant and ordered by time, e.g. 1 data vector is recorded every 10 milliseconds. 
Each $t_i$ consists of $d$ dimensions (also referred to as variates or channels), corresponding to the outputs of individual sensors. When $d = 1$, the data are typically described as a univariate time series (UTS), and when $d > 1$, as a multivariate time series (MTS). The resulting values—also called data points or measurements—are equi-distant, i.e., sampled at uniform time intervals, producing an ordered temporal sequence; for example, one time-series vector may be recorded every 10 milliseconds.



\begin{definition}
Given a TS $T$, a \emph{subsequence} $T_{s,e}$ of $T$ with start offset $s$ and end offset $e$ is the $d$-dimensional slice of contiguous observations from $T$ at position $s$ to position $e$, i.e., $T_{s,e} = (\vec{t}_s,\dots,\vec{t}_e)$ with $1\leq s \leq e \leq n$. The length of $T_{s,e}$ is $|T_{s,e}| = e-s+1$.
\end{definition}

% We use the terms \emph{subsequence} and \emph{window} interchangeably, and refer to their length as the \emph{width}. A state from $S$ yields its observable properties as a subsequence with shapes or statistics distinguishable from others. We call these core structures \emph{temporal patterns}, since they provide the necessary information to recognize the same state and distinguish it from others. Temporal patterns can drift or suddenly change over time, indicating a switch from one process state to another. Note, however, that local parts of channels contribute differently to the signal, for instance in amplitude.
In this paper, we adopt the terms \emph{subsequence} and \emph{window} interchangeably, and denote their length by the \emph{width}. Each state in $S$ manifests itself through characteristic portions of the segments whose shapes or statistical distribution significantly differ from those generated by other states. 
These core structures, which we refer to as \emph{temporal patterns}, encode the information required to identify the underlying state and make separation. 
Such patterns may evolve gradually or undergo abrupt changes, both of which indicate a transition between states of the process. 
Moreover, the influence of individual channels can vary across time, with local regions differing in their contribution to the signal, such as through channel-specific amplitude differences.

\begin{definition}
Given a TS $T$ of size $n$ that captures a process $P = (S,R)$, the corresponding state sequence $Q = (s_{t_1},\dots,s_{t_n}) \in S^n$ contains the states $s_{t_i} \in S$ that are captured at time points $t_i \in T$.
\end{definition}

% A \emph{change point} (CP) denotes an offset $i \in [1,\dots,n]$ for $t_i \in T$ that corresponds to a transition between states $s_{t_{i-1}}$ to $s_{t_i}$ in $Q$, where $(s_{t_{i-1}},s_{t_i}) \in R$ and $s_{t_{i-1}} \neq s_{t_i}$. For notational convenience, we consider the first and last values in $T$ as CPs. We call the subsequence between two CPs a \emph{segment} with variable size. A \emph{segmentation} of $T$ is the ordered sequence of CPs in $T$, i.e., $t_{i_{1}},\dots,t_{i_{n}}$ with ${1\leq i_1<\dots<i_n\leq n}$ at which the process $P$ changes state. 
A \emph{change point} (CP) represents an offset index $i \in \{1,\dots,n\}$ such that the associated observations $t_{i-1}$ and $t_i$ correspond to different states in $Q$; that is, $(s_{t_{i-1}}, s_{t_i}) \in R$ and $s_{t_{i-1}} \neq s_{t_i}$. 
For simplicity, we also treat the first and last elements of $T$ as CPs. 
The subsequence delimited by two consecutive CPs is referred to as a \emph{segment} and may vary in length. 
A \emph{segmentation} of $T$ is the ordered set of CP indices, $t_{i_1}, \dots, t_{i_n}$ with $1 \leq i_1 < \dots < i_n \leq n$, marking the positions where the process $P$ undergoes a state transition.



\section{Time Series Segmentation} \label{sec:tss}

% Survey: http://link.springer.com/10.1007/s10115-016-0987-z
% Survey: https://arxiv.org/pdf/1801.00718
% Survey: https://www.techscience.com/cmc/v80n2/57650

% TS segmentation (TSS) is a common preprocessing step between data collection~\cite{Reiss2012Creating} and knowledge discovery from TS~\cite{Matsubara2014AutoPlait}. It allows inferring the latent states of an underlying process by analysing sensor measurements, as signal shifts from one segment to another are assumed to be caused by state changes in the process being monitored, such as a transition from one human activity to another or from one machine state to another. TSS aims to partition a given TS into consecutive regions such that each region is homogeneous in itself yet sufficiently different from the neighbouring regions. It is typically performed by focusing on the detection of change points (CPs) separating segments~\cite{Aminikhanghahi2017Survey}. State-of-the-art methods for TSS rely on global statistics of the TS, value distributions, densities or learned features~\cite{Truong2020Selective}, and often exhibit a high computational complexity. Recent accurate contributions, e.g. FLUSS~\cite{Gharghabi2018Domain} or ClaSP~\cite{Ermshaus2023ClaSP}, are quadratic in runtime regarding the TS length.
Time series segmentation (TSS) is a widely used preprocessing step situated between data acquisition~\cite{Reiss2012Creating} and downstream knowledge discovery from time series~\cite{Matsubara2014AutoPlait}. 
TSS focuses on recovering the latent states of an underlying process from the sensor measurements, under the assumption that changes in the signal across adjacent regions correspond to transitions in the monitored process—for example, switching from one human activity to another or moving between operating conditions of a machine. 
The goal of TSS is to partition a time series into consecutive intervals that are internally homogeneous, yet sufficiently distinct from neighboring intervals. 
Most approaches achieve this by identifying \emph{change points} (CPs) that delineate segment boundaries~\cite{Aminikhanghahi2017Survey}. 
State-of-the-art TSS techniques typically rely on global statistical properties of the time series—such as value distributions, density characteristics, or learned feature representations~\cite{Truong2020Selective}—and many of them incur substantial computational costs. 
Recent high-accuracy methods, including FLUSS~\cite{Gharghabi2018Domain} and ClaSP~\cite{Ermshaus2023ClaSP}, exhibit quadratic runtime in the time series length.


% CPD is often approached by first fixing the number $C$ of segments; the task then becomes finding $C-1$ splits (change points) in a TS. In such a setting, $C$ is a hyper-parameter that is as difficult to infer from the data as in finding the correct number of clusters in clustering algorithms~\citep{Nguyen2015Survey}. Segmentation, i.e., finding subsequences of a TS produced by the same state of the underlying process, is directly related to CPD and defined as follows.
In most CPD settings, the number of segments $C$ is assumed to be set up or fixed in advance, while the objective is to determine the $C-1$ change points that yield this segmentation. Notably, choosing an appropriate value of $C$ in practice is difficult without prior knowledge, which resembles the challenge of specifying the number of clusters in clustering~\citep{Nguyen2015Survey}. 
Segmentation, the task of identifying subsequences generated by the same state in the process, is tightly connected to CPD. We define this task as follows.




\begin{definition}
The problem of time series segmentation (TSS) is to find a meaningful segmentation of a given TS, such that the CPs between two subsequent segments capture state changes in the observed process. 
\end{definition}

% Naturally, an algorithm which produced $C-1$ change points induces a segmentation of $C$ segments and vice versa. Note that we make no assumptions regarding the frequency of occurrences of states, i.e., a process may start from a state $A$, then shift to $B$, back to $A$ etc. 
Segmentation and change points are tightly connected in practice. Specifically,  identifying $C-1$ change points yields a segmentation with $C$ segments on the entire time series, and vice versa. It is noteworthy that no assumptions are imposed on the ordering or recurrence of states; for example, a process may alternate freely between states $A$ and $B$ without restriction.


\section{Time Series State Detection} \label{sec:tssd}

% TODO: talk more about how state detection extends segmentation

% Survey: https://www.techscience.com/cmc/v80n2/57650

% The task of recovering the sequence of discrete state labels from a TS of observations is called time series state detection (TSSD). It acts as a complex, unsupervised preprocessing step between data collection and TS knowledge discovery~\cite{Wang2024Unsupervised}. TSSD annotates each data point in a TS with a label that corresponds to a state in the data-generating process. Consecutive stretches of the same label mark segments, while different neighbouring labels indicate CPs. TSSD requires the analysis of observations to identify signal shifts, which are assumed to result from state changes in the observed process. This creates consecutive segments of data points that are homogeneous within themselves, yet sufficiently distinct from their neighbours. These segments are further compared to each other to assign equal labels to those sharing the same state and unequal labels to those representing different states. State labels from segments are then propagated to individual data points to form the annotation, a state sequence. Contrary to supervised problems, such as TS classification (TSC), the labels in TSSD (e.g. 0,1,2) are discovered by an algorithm, abstract, and primarily used to differentiate observations based on state affiliation. They can be mapped to semantic labels (e.g. walk, jog, run) when appropriate domain knowledge is available~\cite{Ahad2021IoT}.

Time series state detection (TSSD) refers to the task of reconstructing the sequence of discrete state labels which reflect an observed time series. 
It serves as an unsupervised preprocessing stage positioned between raw data acquisition and higher-level knowledge discovery~\cite{Wang2024Unsupervised}. 
TSSD assigns each data observation a label representing the underlying process state at that time point. 
Sequences of identical states lead to segments, whereas label changes between consecutive observations indicate change points (CPs).
The procedure requires analysing the temporal measurements to identify alterations in the signal that are attributed to transitions in the process being monitored. 
This produces contiguous regions of observations that are internally coherent yet sufficiently distinct from neighbouring regions. 
Segments exhibiting similar structural or statistical characteristics are grouped together and given the same state label, while segments reflecting different behaviours receive different labels. 
After segment-level decisions are made, the resulting labels are propagated back to individual time stamps to form the complete state sequence.
In contrast to supervised tasks such as time series classification (TSC), the labels produced by TSSD (e.g., 0, 1, 2) are not provided a priori. 
Typically, the states' labels are discovered automatically, function as abstract identifiers of process states, and primarily serve to distinguish observations according to state membership. 
Where domain expertise is available, these abstract labels may subsequently be mapped to semantic events or activities, such as \emph{walk}, \emph{jog}, \emph{run}~\cite{Ahad2021IoT}.

\begin{definition}
The problem of \emph{time series state detection} (TSSD) is to recover the latent state sequence $Q$ of a process $P$, only by analysing the time series $T$, emitted by $P$.
\end{definition}

% The TSSD problem, as defined here, is unsupervised and twofold: We need to find a segmentation of $T$ that captures the state transitions $R$, as well as the distinct states $S$ from all segments, in order to predict a state sequence $\hat{Q}$. An optimal result yields a $\hat{Q}$ that is isomorphic to $Q$, because an algorithm does not have access to the actual state labels. To evaluate the quality of a predicted state sequence, we can measure its alignment with the ground truth annotated by domain experts — e.g., by inspecting Covering~\cite{Burg2020Evaluation} or the mutual information~\cite{Nguyen2010Information} of segments.

As formulated in this work, TSSD is an unsupervised problem comprising two intertwined tasks. 
First, a segmentation of $T$ must be derived that reflects the state transitions encoded in $R$. 
Second, the distinct states $S$ represented across the segments must be identified in order to construct a predicted state sequence $\hat{Q}$. 
Since the true state labels are not accessible during the training stage, the best achievable outcome is a prediction $\hat{Q}$ that is isomorphic to the ground-truth sequence $Q$. 
Given a well-trained model, the quality of predictions is typically evaluated against annotations (e.g., typically annotated by domain experts), for example using Covering~\cite{Burg2020Evaluation} or mutual information measures~\cite{Nguyen2010Information} to quantify the alignment.

% A popular approach to TSSD is a combination of TS segmentation (TSS) and clustering techniques~\cite{Wang2024Unsupervised}. A baseline approach could, for example, use the ClaSP (Classification Score Profile) algorithm~\cite{Ermshaus2023ClaSP} to compute a TSS and cluster the resulting segments using Time2Feat~\cite{Bonifati2023Interpretable} to predict state labels.

To illustrate, consider an activity sequence in which a subject alternates between waiting and walking, as reported in~\cite{Reiss2011Towards}. 
The TSSD task consists of annotating the subject’s triaxial acceleration signal—e.g., measured at the left calf—with a sequence of abstract state identifiers: one label for waiting, another for walking, then the waiting label again, and so forth, thereby reconstructing the latent progression of activities.
A widely adopted line of work addresses TSSD by pairing time series segmentation (TSS) with clustering~\cite{Wang2024Unsupervised}. 
One representative method applies the ClaSP (Classification Score Profile) algorithm~\cite{Ermshaus2023ClaSP} to obtain a segmentation prediction, where a sequence of state labels can be subsequently grouped by a Time2Feat~\cite{Bonifati2023Interpretable} based clustering process.

% An example is an activity routine, in which a human subject first waits, then walks, waits again, and goes on to perform other motions, as studied in~\cite{Reiss2011Towards}. The TSSD task would be to annotate e.g. the triaxial acceleration signal of the subject's left calf, with a state sequence of zeros (for waiting), ones (for walking), zeros again (for waiting), and so forth; capturing the latent process.


\section{Differences of Approaches} \label{sec:differences}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/tax-Seg.pdf}
    \caption{Time Series Segmentation Taxonomy.}
    \label{fig:taxonomy}
\end{figure*}

% Talk about the differences of approaching CPD vs TSS; CPD research mainly focuses on changes in distribution, while TSS research focuses on changes in shape. Underlying problem is the same; CPD does not assume/exploit dependencies between observations (e.g. patterns), while TSS does. This leads to a gap between communities (statistics vs data mining / machine learning) as well as differences in evaluation (theoretical properties and simulations vs real-world benchmarks).  


\section{Benchmark} \label{sec:benchmark}

\subsection{Methods} \label{sec:benchmark_methods}

\paragraph{Change Point Detection / Change in Distribution:}

\begin{enumerate}
    \item AMOC~\cite{Hinkley1970Inference}
    
    \item BinSeg~\cite{Scott1974Cluster}
    
    \item BOCD~\cite{Adams2007Bayesian} calculates the probability distribution of data points since the last change using a recursive message-passing algorithm to infer the most recent change point. Draayer et al.~\cite{Draayer2021Reevaluating} extend this idea for short gradual changes.
    
    \item BottumUp~\cite{Keogh2001Online}
    
    \item DynP~\cite{Truong2020Selective}
    
    \item EAgglo~\cite{Matteson2013Nonparametric}
    
    \item GGS~\cite{Hallac2016Greedy}
    
    \item iCID~\cite{Cao2022Detecting}
    
    \item KCPD~\cite{Truong2020Selective}
    
    \item PELT~\cite{Killick2012Optimal}
    
    \item Prophet~\cite{Taylor2018Forecasting}

    \item TIRE~\cite{DeRyck2020Change}
    
    \item TS-CP2~\cite{Deldari2020Time}
    
    \item Window~\cite{Truong2020Selective}
\end{enumerate}



\paragraph{Time Series Segmentation / Change in Shape}

\begin{enumerate}
    \item ClaSP~\cite{Ermshaus2023ClaSP} frames TSS as a collection of self-supervised subsequence classification problems and reports the segmentation with the highest cross-validation performance.
    
    \item ESPRESSO~\cite{Deldari2020ESPRESSO} is a variant of FLUSS that extends the arc curve with TS chains~\cite{Zhu2017Matrix} and weighs it with positional subsequence information in order to detect reoccurring segments. It also uses a more sophisticated entropy-based segmentation procedure. 
    
    \item FLUSS~\cite{Gharghabi2018Domain} uses the proximity of a window to the most similar other window to create an arc curve, which is a vector that contains for each index $i$ the number of \emph{arcs} that cross over $i$. Local minima of this number indicate boundaries (change points) of homogenous regions.
    
    \item Hidalgo~\cite{Allegra2019Data}
    
    \item IGTS~\cite{Sadri2017Information}
    
    \item PATSS~\cite{Carpentier2024Pattern}
    
    \item tGlad~\cite{Imani2023tGLAD}
    
\end{enumerate}


\paragraph{Time Series State Detection:}

\begin{enumerate}
    \item AutoPlait~\cite{Matsubara2014AutoPlait}
    
    \item CLaP~\cite{Ermshaus2025CLaP} uses self-supervised change analysis~\cite{Hido2008Unsupervised} to exploit the predictive power of TS classification algorithms and merges confused classes as long as their cgain increases, which clusters segments automatically in label space rather than in feature space.
    
    \item E2USD~\cite{Lai2024E2Usd} implements a deep-learning TSSD architecture, transforming each sliding window through compression, trend–seasonal decomposition and contrastive learning into a compact embedding that is subsequently clustered via DPGMM.
    
    \item HDP-HSMM~\cite{Nagano2018Sequence} incorporates explicit-duration semi-Markovian properties to model flexible HMM state durations and employs a hierarchical Dirichlet process prior to learn the number of states from the TS directly.
    
    \item HMM~\cite{Viterbi1967Error}
    
    \item TICC~\cite{Hallac2017Toeplitz} characterizes TS using a sliding window. Each subsequence belongs to a cluster that is characterized by a correlation network. The assignment of subsequences and the update of cluster parameters are iteratively refined using an expectation maximization algorithm.
    
    \item Time2State~\cite{Wang2023Time2State} trains a deep learning encoder by minimizing (maximizing) distances of intra-state (inter-state) subsequences. The resulting embedding is clustered using DPGMM to assign subsequences to state labels and to automatically learn their amount.
    
\end{enumerate}

% Other

Random

\subsection{Datasets}

\begin{longtable}{l c c c c c m{1cm}}
\toprule
\textbf{Dataset} & \textbf{\#} & \textbf{Length} & \textbf{D} & \textbf{K} & \textbf{CPs} & \textbf{Example Series} \\
\midrule
\endfirsthead
\toprule
\textbf{Dataset} & \textbf{\#} & \textbf{Length} & \textbf{D} & \textbf{K} & \textbf{CPs} & \textbf{Example Series} \\
\midrule
\endhead
\bottomrule
\endfoot
actrectut & 2 & 31.4-32.6k & 10 & 6 & 42 & \includegraphics[width=8cm, height=0.7cm, keepaspectratio=false]{figures/time_series_samples/actrectut.png} \\
has & 250 & 0.3-41.5k & 6-9 & 1-12 & 0-14 & \includegraphics[width=8cm, height=0.7cm, keepaspectratio=false]{figures/time_series_samples/has.png} \\
mocap & 9 & 4.6-10.6k & 4 & 3-9 & 5-10 & \includegraphics[width=8cm, height=0.7cm, keepaspectratio=false]{figures/time_series_samples/mocap.png} \\
pamap2 & 9 & 8.5-447.0k & 9 & 2-13 & 2-27 & \includegraphics[width=8cm, height=0.7cm, keepaspectratio=false]{figures/time_series_samples/pamap2.png} \\
skab & 34 & 0.7-1.3k & 8 & 2 & 1-2 & \includegraphics[width=8cm, height=0.7cm, keepaspectratio=false]{figures/time_series_samples/skab.png} \\
tssb & 75 & 0.2-20.7k & 1 & 1-7 & 0-8 & \includegraphics[width=8cm, height=0.7cm, keepaspectratio=false]{figures/time_series_samples/tssb.png} \\
usc-had & 70 & 25.4-56.3k & 6 & 12 & 11 & \includegraphics[width=8cm, height=0.7cm, keepaspectratio=false]{figures/time_series_samples/usc-had.png} \\
utsa & 32 & 2.0-40.0k & 1 & 2-3 & 1-2 & \includegraphics[width=8cm, height=0.7cm, keepaspectratio=false]{figures/time_series_samples/utsa.png} \\
\end{longtable}

\subsection{Methodology}
\label{sec:methodology}

To provide a comprehensive assessment of time series segmentation and state detection algorithms, we designed a systematic evaluation framework comprising two distinct experimental strategies and two supervision regimes.

\subsection{Experimental Strategies}
\label{sec:exp_strategy}
We evaluate the algorithms using two approaches to measure both their baseline usability and their potential peak performance:

\begin{itemize}[leftmargin=10pt]
    \item \textbf{Standard Parameter Evaluation:} In this setting, algorithms are executed using their default hyper-parameters as specified in their original implementations or documentation. This strategy assesses the ``out-of-the-box'' performance and generalizability of the methods without requiring user intervention or domain-specific tuning.
    \item \textbf{Grid Search Optimization:} To evaluate the algorithms' sensitivity to hyper-parameters and their maximum potential performance, we conduct a grid search over a predefined space of parameters. This tuning process identifies the optimal configuration for each dataset, providing an upper bound on the achievable accuracy for each method.
\end{itemize}

\subsection{Supervision Regimes}
\label{sec:supervision_regime}
We further categorize our experiments based on the amount of prior knowledge available to the algorithms:

\begin{itemize}
    \item \textbf{Default:} Algorithms operate in a fully blind manner, inferring all structural properties, including the number of segments or states, solely from the data.
    \item \textbf{Guided:} In this regime, we guide the models by providing oracle information regarding the ground truth structure. Specifically, we supply the expected number of change points, segments, or states to any algorithm capable of accepting such constraints. This allows us to decouple the challenge of estimating the number of events from the challenge of localization and state identification.
\end{itemize}

\subsection{Multivariate Adaptation}
The benchmark datasets encompass both univariate and multivariate time series, whereas the evaluated algorithms often possess inherent modality constraints (i.e., supporting only univariate or only multivariate input). While multivariate-only algorithms cannot be applied to univariate data, we employ two distinct strategies to adapt univariate-only algorithms for multivariate time series:

\begin{itemize}
    \item \textbf{L2 Norm Reduction:} We reduce the multivariate signal to a univariate representation by computing the L2 norm of the vector at each time step. This transformation preserves the magnitude of simultaneous changes across dimensions while enabling compatibility with univariate detectors.
    \item \textbf{Ensemble Aggregation:} We execute the univariate algorithm on each dimension independently. The resulting change points from all dimensions are pooled and clustered based on temporal proximity using a tolerance window. Clusters are ranked by the number of contributing dimensions (consensus), and the final change point locations are derived from the median position of the top-ranked clusters.
\end{itemize}

\subsection{Evaluation Metrics}
We employ a comprehensive suite of metrics to evaluate segmentation quality from multiple perspectives, including boundary localization and state clustering. In addition to standard measures, we utilize the Weighted Adjusted Rand Index (WARI) and the State Matching Score (SMS)~\cite{addchavelli2025}. These recently proposed metrics provide a more robust assessment of temporal alignment and state detection performance compared to traditional metrics.

\subsection{Computational Resources}
All experiments were conducted on a high-performance computing cluster composed of Cascade Lake nodes equipped with dual Intel Xeon 5218 processors (2×16 cores, 2.4 GHz, with hyperthreading) and 192 GB of RAM. To ensure a consistent evaluation environment, each experimental trial was allocated 4 CPU cores and 16 GB of RAM, with a maximum execution time of 24 hours.



\section{Experimental Evaluation} \label{sec:experiments}

\begin{figure}[t]
    \centering
    \begin{minipage}{8cm}
        \centering
        \includegraphics[width=0.9\columnwidth]{figures/cd_diagram_default_bidirectional_covering_score.pdf}
    \end{minipage}
    \begin{minipage}{8cm}
        \centering
        \includegraphics[width=0.9\columnwidth]{figures/boxplot_default_bidirectional_covering_score.pdf}
    \end{minipage}
    \caption{Bidirectional Covering ranks (top) and box plot (bottom) on $472$ TS for $16$ TSS competitors.\label{fig:cd_benchmark_covering}}
\end{figure}

\begin{figure}[t]
    \centering
    \begin{minipage}{8cm}
        \centering
        \includegraphics[width=0.9\columnwidth]{figures/cd_diagram_default_state_matching_score_score.pdf}
    \end{minipage}
    \begin{minipage}{8cm}
        \centering
        \includegraphics[width=0.9\columnwidth]{figures/boxplot_default_state_matching_score_score.pdf}
    \end{minipage}
    \caption{State Matching ranks (top) and box plot (bottom) on $472$ TS for $5$ TSSD competitors.\label{fig:cd_benchmark_sms}}
\end{figure}

\subsection*{RQ1: Best overall algorithm}

We evaluated the average Bidirectional Covering and State Matching Score ranks and summary statistics of $16$ TSS and $5$ TSSD competitors on $472$ TS from $7$ benchmarks to identify the overall best-performing methods for both problems.

\textbf{TSS:} Figure~\ref{fig:cd_benchmark_covering} (top) presents the ranking results for TSS under the default setting. GGS, ClaSP, CLaP, E2USD, and iCID form a clique with the lowest average ranks and only marginal differences, followed by Time2State, AMOC, and FLUSS. These top-8 methods significantly outperform the bottom-8 worst-performing approaches. Notably, six algorithms perform worse than the random baseline in the default setting. In the guided setting, only three methods perform worse than random, and the ranking changes substantially. Nevertheless, CLaP and ClaSP remain among the top-3 approaches. Note that CLaP internally relies on ClaSP for TSS, but differs due to its subsequent merging procedure. When considering the seven benchmarks individually under the default setting, no single approach consistently achieves top-ranking results. Instead, rankings vary notably across benchmarks compared to the overall results. This suggests that no single algorithm reliably outperforms all others across diverse data set characteristics. The boxplot in Figure~\ref{fig:cd_benchmark_covering} (bottom) supports these findings. The top-8 algorithms exhibit similar median Covering scores and dispersion, whereas the bottom-8 methods show a pronounced performance drop and a higher number of outliers.

\textbf{TSSD:} The ranking results for TSSD under the default setting in Figure~\ref{fig:cd_benchmark_sms} (top) show that E2USD and CLaP significantly outperform Time2State, our random baseline, and HDP-HSMM. Interestingly, the two best-performing methods are also among the top approaches for TSS, indicating that their strong segmentation performance transfers well to state detection. In the guided setting, CLaP significantly outperforms E2USD and all other competitors. When analyzing individual benchmarks under the default setting, CLaP achieves the best performance on $4$ out of $7$ benchmarks (HAS, SKAB, TSSB, UTSA), whereas E2USD attains the highest performance only on the ActRecTut benchmark. This suggests that CLaP provides accurate state predictions across diverse problem settings. While HAS and SKAB consist of multivariate human activity and machine operation data, TSSB and UTSA contain univariate cross-domain TS. These observations are supported by the boxplot in Figure~\ref{fig:cd_benchmark_sms} (bottom), which shows CLaP achieving the highest median SMS performance, followed by E2USD and Time2State. In contrast, the random baseline and HDP-HSMM exhibit substantially lower performance with larger drops.

\textbf{RQ1 Conclusion:} Overall, our experiments do not identify a single TSS algorithm that significantly outperforms all others and consistently delivers robust results across diverse problem settings. This indicates that practitioners should select TSS methods based on the specific characteristics of their application. Nevertheless, several algorithms demonstrate strong overall performance, including GGS, CLaP (using ClaSP), E2USD, and iCID, despite relying on fundamentally different methodological principles. For TSSD, CLaP and E2USD emerge as the most reliable choices, as both significantly outperform competing approaches. In particular, CLaP achieves the best rankings on the majority of benchmark data sets, making it a strong default choice for both TSS and TSSD. 

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\columnwidth]{figures/tuning_gain_aligned_bidirectional_covering_score.pdf}
    \caption{Bidirectional Covering performance across $24$ TSS competitors, with varying supervision including ``guided'' (oracle information of number of change points or states) and ``grid'' (grid search over predefined space of parameters) .\label{fig:barplot_tss_tunning_supervision}}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\columnwidth]{figures/tuning_gain_aligned_state_matching_score.pdf}
    \caption{State Matching performance across $6$ TSSD competitors, with varying supervision including ``guided'' (oracle information of number of change points or states) and ``grid'' (grid search over predefined space of parameters) .\label{fig:barplot_tssd_tunning_supervision}}
\end{figure}







\subsection*{RQ3: Supervision}

As illustrated in Sections~\ref{sec:exp_strategy} and \ref{sec:supervision_regime}, we conduct a granular analysis of the effects of supervision to address two primary and related research questions: (i) \textit{Does external supervision significantly improve the performance of TSS and TSSD algorithms?} and (ii) \textit{If so, what form of supervision is most effective?} To answer these questions, we evaluate four distinct combinations of supervision strategies, as depicted in Figure~\ref{fig:barplot_tss_tunning_supervision} and ~\ref{fig:barplot_tssd_tunning_supervision}: (i) \textit{Default} (Light Blue): Algorithms utilize their default parameterizations without external guidance or prior knowledge; \textit{Guided} (Light Orange): Algorithms receive oracle information regarding the ground-truth structure (e.g., the exact number of change points or states) but do not undergo additional parameter tuning; \textit{Grid\_Default} (Dark Blue): Algorithms identify the optimal configuration for each dataset through grid search, though no oracle structural information is provided; and \textit{Grid\_Guided} (Dark Orange): A hybrid approach where algorithms leverage both grid-search parameter optimization and oracle information regarding the number of change points or states.

Overall, we observe that algorithms provided with oracle structural information (\textit{Guided}) consistently outperform those without such guidance (\textit{Default}), a trend that persists across both time-series segmentation (TSS) and time-series state detection (TSSD) experiments. For TSSD tasks, this form of supervision appears indispensable; the exclusion of structural oracle information can result in a significant $20\%$–$40\%$ performance degradation. A similar trend is observed for most TSS algorithms as well, particularly for highly performant methods such as ClaSP, CLaP, and KCPD. The parameter tuning strategies, \textit{Grid\_Default} and \textit{Grid\_Guided}, yield generic performance improvements across all methods, albeit at a substantially higher computational cost. Among these, \textit{Grid\_Guided} consistently achieves the highest overall accuracy. However, the marginal benefit of grid-search parameter tuning varies significantly depending on the specific algorithm and task. For instance, TSSD algorithms and several TSS methods—including ClaSP, CLaP, E2USD, and Time2State—demonstrate robust performance even in the absence of intensive parameter tuning. In contrast, methods such as DYNP, KCPD, and BinSeg benefit substantially from such supervision. Notably, for certain methods, the parameter tuning process allows them to achieve performance levels comparable to those of oracle-guided versions, even without oracle structural information. These results underscore the significant impact of both structural guidance and hyperparameter optimization on representation accuracy.

\textbf{RQ3 Conclusion:}
Through this exploratory study, we demonstrate the positive impact of varying levels of supervision on both TSS and TSSD algorithms. Overall, guidance derived from oracle information—such as the exact number of change points or states—plays a significant role in improving performance across most methods without incurring additional computational overhead. Consequently, this form of supervision is particularly advantageous when expert or prior knowledge is available.
Furthermore, hyperparameter optimization via a grid-search process enhances the performance of most algorithms, albeit at a higher computational cost. Notably, we observe that the magnitude of this benefit varies significantly across different methods. Algorithms such as ClaSP and CLaP appear remarkably stable and robust to default parameter selection. In contrast, methods such as KCPD, BinSeg, and BottomUp exhibit a greater dependency on such supervision, yielding more substantial performance gains when parameters are finely tuned to reach the full potential. 


\subsection*{RQ4: Scalability}


\begin{figure}[t]
    \centering
    % \includegraphics[width=1.0\columnwidth]{figures/runtime_all_datasets_mode.pdf}
    \includegraphics[width=1.0\columnwidth]{figures/runtime_all_datasets_single.pdf}
    \caption{Runtime by method over all datasets.\label{fig:runtime_by_method}}
\end{figure}


Figure~\ref{fig:scalability-top12-runtime-vs-covering} compares duration in seconds across increasing time series lengths for each dataset, considering only the TOP-12 competitors. TODO


\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\columnwidth]{figures/runtime_top12_scatter_runtime.pdf}
    \caption{Runtime grouped by length and dataset for TOP-12 competitors using bidirectional covering scores.\label{fig:scalability-top12-runtime-vs-length}}
\end{figure}

Figure~\ref{fig:scalability-top12-runtime-vs-covering} compares bi-directional covering scores across increasing time series lengths for each dataset, considering only the TOP-12 competitors. No consistent trend is observable: longer time series do not systematically lead to higher or lower scores.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\columnwidth]{figures/runtime_top12_scatter_covering.pdf}
    \caption{Bidirectional-covering score grouped by length and dataset for TOP-12 competitors using bidirectional covering scores.\label{fig:scalability-top12-runtime-vs-covering}}
\end{figure}


\textbf{RQ4 Conclusion:}



\section{Conclusion} \label{sec:conclusion}

% \begin{acks}
% \end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{main}

% \appendix
\end{document}
